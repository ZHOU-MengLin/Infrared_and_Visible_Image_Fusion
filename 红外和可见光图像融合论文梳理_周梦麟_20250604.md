# 红外和可见光图像融合论文梳理

>  Infrared and visible image fusion

[综述：VIFB: A Visible and Infrared Image Fusion Benchmark](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2002.03322)

**一、传统方法**

1.  ADF[Fusion of Infrared and Visible Sensor Images Based on Anisotropic Diffusion and KL Transform](https://link.zhihu.com/?target=https%3A//uk.mathworks.com/matlabcentral/fileexchange/63591-fusion-of-infrared-and-visible-sensor-images-based-on-anisotropic-diffusion-and-kl-transform%3Fs_tid%3Dprof_contriblnk)

2. CBF[Image fusion based on pixel significance using cross bilateral filter - File Exchange - MATLAB Central](https://ww2.mathworks.cn/matlabcentral/fileexchange/43781-image-fusion-based-on-pixel-significance-using-cross-bilateral-filter))

3. FPDE[Multi-sensor Image Fusion based on Fourth Order Partial Differential Equations](https://link.zhihu.com/?target=https%3A//uk.mathworks.com/matlabcentral/fileexchange/63570-multi-sensor-image-fusion-based-on-fourth-order-partial-differential-equations%3Fs_tid%3Dprof_contriblnk)

4. GFCE[Context-Enhance-via-Fusion](https://link.zhihu.com/?target=https%3A//github.com/bitzhouzq/Context-Enhance-via-Fusion)

5. GFF[http://xudongkang.weebly.com/](https://link.zhihu.com/?target=http%3A//xudongkang.weebly.com/)

6. GTF[https://github.com/jiayi-ma/GTF](https://link.zhihu.com/?target=https%3A//github.com/jiayi-ma/GTF)

7. HMSD_GF[https://github.com/bitzhouzq/Context-Enhance-via-Fusion](https://link.zhihu.com/?target=https%3A//github.com/bitzhouzq/Context-Enhance-via-Fusion)

8. Hybrid_MSD[https://github.com/bitzhouzq/Hybrid-MSD-Fusion](https://link.zhihu.com/?target=https%3A//github.com/bitzhouzq/Hybrid-MSD-Fusion)

9. IFEVIP[https://github.com/uzeful/Infrared-and-Visual-Image-Fusion-via-Infrared-Feature-Extraction-and-Visual-Information-Preservation](https://link.zhihu.com/?target=https%3A//github.com/uzeful/Infrared-and-Visual-Image-Fusion-via-Infrared-Feature-Extraction-and-Visual-Information-Preservation)

10. LatLRR[https://github.com/hli1221/imagefusion_Infrared_visible_latlrr](https://link.zhihu.com/?target=https%3A//github.com/hli1221/imagefusion_Infrared_visible_latlrr)

11. MGFF[Multi-scale Guided Image and Video Fusion](https://link.zhihu.com/?target=https%3A//www.mathworks.com/matlabcentral/fileexchange/72451-multi-scale-guided-image-and-video-fusion%3Fs%255C_tid%3Dprof%255C_contriblnk)

12-14. MST_SR, NSCT_SR, RP_SR[https://github.com/yuliu316316/MST-SR-Fusion-Toolbox](https://link.zhihu.com/?target=https%3A//github.com/yuliu316316/MST-SR-Fusion-Toolbox)

15. MSVD[Image Fusion Technique using Multi-resolution Singular Value Decomposition](https://link.zhihu.com/?target=https%3A//uk.mathworks.com/matlabcentral/fileexchange/38802-image-fusion-technique-using-multi-resolution-singular-value-decomposition)

16. TIF[Two-scale image fusion of visible and infrared images using saliency detection](https://link.zhihu.com/?target=https%3A//uk.mathworks.com/matlabcentral/fileexchange/63571-two-scale-image-fusion-of-visible-and-infrared-images-using-saliency-detection%3Fs_tid%3Dprof_contriblnk)

17. VSMWLS[https://github.com/JinleiMa/Image-fusion-with-VSM-and-WLS](https://link.zhihu.com/?target=https%3A//github.com/JinleiMa/Image-fusion-with-VSM-and-WLS)
18. 

**二、基于深度学习的方法**

CNN[https://github.com/yuliu316316/CNN-IV-Fusion](https://link.zhihu.com/?target=https%3A//github.com/yuliu316316/CNN-IV-Fusion)

DLF[https://github.com/hli1221/imagefusion_deeplearning](https://link.zhihu.com/?target=https%3A//github.com/hli1221/imagefusion_deeplearning)

ResNet[https://github.com/hli1221/imagefusion_resnet50](https://link.zhihu.com/?target=https%3A//github.com/hli1221/imagefusion_resnet50)

图像融合系列博客还有：

1.  图像融合论文及代码整理最全大合集参见：[图像融合论文及代码整理最全大合集](https://blog.csdn.net/fovever_/article/details/124650534?spm=1001.2014.3001.5502)
2.  图像融合综述论文整理参见：[图像融合综述论文整理](https://blog.csdn.net/fovever_/article/details/124409916?spm=1001.2014.3001.5502)
3.  图像融合评估指标参见：[红外和可见光图像融合评估指标](https://blog.csdn.net/fovever_/article/details/106906768)
4.  图像融合常用数据集整理参见：[图像融合常用数据集整理](https://blog.csdn.net/fovever_/article/details/124410445?spm=1001.2014.3001.5502)
5.  通用图像融合框架论文及代码整理参见：[通用图像融合框架论文及代码整理](https://blog.csdn.net/fovever_/article/details/124406720)
6.  基于深度学习的红外和可见光图像融合论文及代码整理参见: [基于深度学习的红外和可见光图像融合论文及代码整理](https://blog.csdn.net/fovever_/article/details/124406594?spm=1001.2014.3001.5501)
7.  更加详细的红外和可见光图像融合代码参见：[红外和可见光图像融合论文及代码整理](https://blog.csdn.net/fovever_/article/details/106585576)
8.  基于深度学习的多曝光图像融合论文及代码整理参见：[基于深度学习的多曝光图像融合论文及代码整理](https://blog.csdn.net/fovever_/article/details/124476866?spm=1001.2014.3001.5502)
9.  基于深度学习的多聚焦图像融合论文及代码整理参见：[基于深度学习的多聚焦图像融合 (Multi-focus Image Fusion) 论文及代码整理](https://blog.csdn.net/fovever_/article/details/124478088)
10.  基于深度学习的全色图像锐化论文及代码整理参见：[基于深度学习的全色图像锐化 (Pansharpening) 论文及代码整理](https://blog.csdn.net/fovever_/article/details/124518130?spm=1001.2014.3001.5502)
11.  基于深度学习的医学图像融合论文及代码整理参见：[基于深度学习的医学图像融合 (Medical image fusion) 论文及代码整理](https://blog.csdn.net/fovever_/article/details/124523422)
12.  彩色图像融合参见: [彩色图像融合](https://blog.csdn.net/fovever_/article/details/124625687?spm=1001.2014.3001.5502)
13.  SeAFusion: 首个结合高级视觉任务的图像融合框架参见：[SeAFusion: 首个结合高级视觉任务的图像融合框架](https://blog.csdn.net/fovever_/article/details/122288699?spm=1001.2014.3001.5501)

| 年份 | 方法            | 发表期刊或会议 | 标题                                                         | 论文                                                         | 代码                                                         | 基础框架 |
| ---- | --------------- | -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------- |
| 2019 | **FusionGAN**   | InfFus         | FusionGAN: A generative adversarial network for infrared and  visible image fusion | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S1566253518301143%3Fvia%3Dihub) | [Code](https://github.com/jiayi-ma/FusionGAN)                | GAN      |
| 2019 | **DenseFuse**   | TIP            | DenseFuse: A Fusion Approach to Infrared and Visible Images  | [Paper](https://ieeexplore.ieee.org/abstract/document/8580578/) | [Code](https://github.com/hli1221/imagefusion_densefuse)     | AE       |
| 2019 | DDcGAN          | IJCAI          | Learning a Generative Model for Fusing Infrared and Visible  Images via Conditional Generative Adversarial Network with Dual  Discriminators | [Paper](https://www.ijcai.org/proceedings/2019/0549.pdf)     | [Code](https://github.com/hanna-xu/DDcGAN)                   | GAN      |
| 2020 | NestFuse        | TIM            | NestFuse: An Infrared and Visible Image Fusion Architecture  Based on Nest Connection and Spatial/Channel Attention Models | [Paper](https://ieeexplore.ieee.org/document/9127964/)       | [Code](https://github.com/hli1221/imagefusion-nestfuse)      | AE       |
| 2020 | DDcGAN          | TIP            | DDcGAN: A dual-discriminator conditional generative  adversarial network for multi-resolution image fusion | [Paper](https://ieeexplore.ieee.org/abstract/document/9031751/) | [Code](https://github.com/hanna-xu/DDcGAN)                   | GAN      |
| 2020 | DIDFuse         | IJCAI          | DIDFuse: Deep Image Decomposition for Infrared and Visible Image Fusion | [Paper](https://arxiv.org/abs/2003.09210)                    | [Code](https://github.com/Zhaozixiang1228/IVIF-DIDFuse)      | AE       |
| 2021 | **RFN-Nest**    | InfFus         | RFN-Nest: An end-to-end residual fusion network for infrared  and visible images | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S1566253521000440%3Fvia%3Dihub) | [Code](https://github.com/hli1221/imagefusion-rfn-nest)      | AE       |
| 2021 | CSF             | TCI            | Classification Saliency-Based Rule for Visible and Infrared  Image Fusion | [Paper](https://ieeexplore.ieee.org/abstract/document/9502544) | [Code](https://github.com/hanna-xu/CSF)                      | AE       |
| 2021 | DRF             | TIM            | DRF: Disentangled Representation for Visible and Infrared  Image Fusion | [Paper](https://ieeexplore.ieee.org/document/9345717/)       | [Code](https://github.com/hanna-xu/DRF)                      | AE       |
| 2021 | SEDRFuse        | TIM            | SEDRFuse: A Symmetric Encoder–Decoder With Residual Block  Network for Infrared and Visible Image Fusion | [Paper](https://ieeexplore.ieee.org/document/9187663)        | [Code](https://github.com/jianlihua123/SEDRFuse)             | AE       |
| 2021 | MFEIF           | TCSVT          | Learning a Deep Multi-Scale Feature Ensemble and an  Edge-Attention Guidance for Image Fusion | [Paper](https://ieeexplore.ieee.org/document/9349250)        |                                                              | AE       |
| 2021 | Meta-Learning   | TIP            | Different Input Resolutions and Arbitrary Output Resolution: A  Meta Learning-Based Deep Framework for Infrared and Visible Image Fusion | [Paper](https://ieeexplore.ieee.org/document/9394791/)       |                                                              | CNN      |
| 2021 | RXDNFuse        | InfFus         | RXDNFuse: A aggregated residual dense network for infrared and  visible image fusion | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S1566253520304152%3Fvia%3Dihub) | [Code](https://github.com/JinyuanLiu-CV/MFEIF)               | CNN      |
| 2021 | STDFusionNet    | TIM            | STDFusionNet: An Infrared and Visible Image Fusion Network  Based on Salient Target Detection | [Paper](https://ieeexplore.ieee.org/document/9416507)        | [Code](https://github.com/Linfeng-Tang/STDFusionNet)         | CNN      |
| 2021 | D2LE            | TIP            | A Bilevel Integrated Model With Data-Driven Layer Ensemble for  Multi-Modality Image Fusion | [Paper](https://ieeexplore.ieee.org/document/9293146)        |                                                              | CNN      |
| 2021 | HAF             | ACM MM         | Searching a Hierarchically Aggregated Fusion Architecture for  Fast Multi-Modality Image Fusion | [Paper](https://dl.acm.org/doi/abs/10.1145/3474085.3475299%3Fcasa_token%3DtT48gSwVWjkAAAAA%3AQaDUAB7nLzWcByiAESzOTAgFjdh5kLxS8J612shuDn3RLLIOcU1AX7AhcvYT9UlGTub1mi85Tws) | [Code](https://github.com/LiuzhuForFun/Hierarchical-NAS-Image-Fusion) | CNN      |
| 2021 | SDDGAN          | TMM            | Semantic-supervised Infrared and Visible Image Fusion via a  Dual-discriminator Generative Adversarial Network | [Paper](https://ieeexplore.ieee.org/document/9623476)        | [Code](https://github.com/WeiWu-WIT/SDDGAN)                  | GAN      |
| 2021 | Detail-GAN      | InfFus         | Infrared and visible image fusion via detail preserving  adversarial learning | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S1566253519300314%3Fvia%3Dihub) | [Code](https://github.com/jiayi-ma/ResNetFusion)             | GAN      |
| 2021 | Perception-GAN  | InfFus         | Image fusion based on  generative adversarial network consistent with perception | [Paper](https://www.sciencedirect.com/science/article/pii/S1566253521000439%3Fcasa_token%3D-LrGkXT12IMAAAAA%3A35WH5dIx-2zWPZ3sTFL3cgjjCK_17nkc0xYVkDXL8Pp51k3DIPtaNT8NWvuKoXlIOoRkKi87AZo0) | [Code](https://github.com/thfylsty/imagefusion_Perceptual_FusionGan) | GAN      |
| 2021 | GAN-FM          | TCI            | GAN-FM: Infrared and Visible  Image Fusion Using GAN With Full-Scale Skip Connection and Dual Markovian  Discriminators | [Paper](https://ieeexplore.ieee.org/document/9573457)        | [Code](https://github.com/yuanjiteng/GAN-FM)                 | GAN      |
| 2021 | AttentionFGAN   | TMM            | AttentionFGAN: Infrared and Visible Image Fusion Using  Attention-Based Generative Adversarial Networks | [Paper](https:////ieeexplore.ieee.org/document/9103116/)     |                                                              | GAN      |
| 2021 | GANMcC          | TIM            | GANMcC: A Generative  Adversarial Network With Multiclassification Constraints for Infrared and  Visible Image Fusion | [Paper](https://ieeexplore.ieee.org/abstract/document/9274337) | [Code](https://github.com/jiayi-ma/GANMcC)                   | GAN      |
| 2021 | MgAN-Fuse       | TIM            | Multigrained Attention Network for Infrared and Visible Image  Fusion | [Paper](https://ieeexplore.ieee.org/document/9216075)        |                                                              | GAN      |
| 2021 | TC-GAN          | TCSVT          | Infrared and Visible Image  Fusion via Texture Conditional Generative Adversarial Network | [Paper](https://ieeexplore.ieee.org/abstract/document/9335976) |                                                              | GAN      |
| 2021 | AUIF            | TCSVT          | Efficient and model-based infrared and visible image fusion via algorithm unrolling | [Paper](https://ieeexplore.ieee.org/abstract/document/9416456/) | [Code](https://github.com/Zhaozixiang1228/IVIF-AUIF-Net)     | AE       |
| 2022 | **TarDAL**      | CVPR           | Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection | [Paper](https://arxiv.org/abs/2203.16220v1)                  | [Code](https://github.com/JinyuanLiu-CV/TarDAL)              | GAN      |
| 2022 | **U2Fusion**    | TPAMI          | U2Fusion: A Unified Unsupervised Image Fusion Network        | [Paper](https://ieeexplore.ieee.org/document/9151265/?arnumber=9151265) | [Code](https://github.com/Linfeng-Tang/SeAFusion)            | CNN      |
| 2022 | **RFNet**       | CVPR           | RFNet: Unsupervised Network for Mutually Reinforcing Multi-modal Image Registration and Fusion | [Paper]([RFNet: Unsupervised Network for Mutually Reinforcing Multi-Modal Image Registration and Fusion](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_RFNet_Unsupervised_Network_for_Mutually_Reinforcing_Multi-Modal_Image_Registration_and_CVPR_2022_paper.pdf)) | [Code](https://github.com/hanna-xu)                          | CNN      |
| 2022 | **SeAFusion**   | InfFus         | **Image fusion in the loop of  high-level vision tasks**: A semantic-aware real-time infrared and visible  image fusion network | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S1566253521002542%3Fvia%3Dihub) | [Code](https://github.com/Linfeng-Tang/SeAFusion)            | CNN      |
| 2022 | PIAFusion       | InfFus         | PIAFusion: A progressive infrared and visible image fusion  network based on illumination aware | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S156625352200032X) | [Code](https://github.com/Linfeng-Tang/PIAFusion)            | CNN      |
| 2022 | UMF-CMGR        | IJCAI          | Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration | [Paper](https://arxiv.org/pdf/2205.11876.pdf)                | [Code](https://github.com/wdhudiekou/UMF-CMGR)               | CNN      |
| 2022 | DetFusion       | ACM MM         | DetFusion: A Detection-driven Infrared and Visible Image Fusion Network | [Paper](https://dl.acm.org/doi/abs/10.1145/3503161.3547902?casa_token=YmZKEkd1zVcAAAAA:wb0HbfopS60BwSh0_QAdHHjwNQ5ZoWxduwHfBd1NzLbMr32AOPcInC4TCbZ5KM9Ly1PbVIGkmg) | [Code](https://github.com/SunYM2020/DetFusion)               | CNN      |
| 2023 | DIVFusion       | InfFus         | DIVFusion: Darkness-free infrared and visible image fusion   | [Paper](https://authors.elsevier.com/c/1g4EB_ZdCkUn0I)       | [Code](https://github.com/Xinyu-Xiang/DIVFusion)             | CNN      |
| 2023 | PSFusion        | InfFus         | Rethinking the necessity of **image fusion in high-level vision tasks**: A practical infrared and visible image fusion network based on progressive semantic injection and scene fidelity | [Paper](https://www.sciencedirect.com/science/article/pii/S1566253523001860) | [Code](https://github.com/Linfeng-Tang/PSFusion)             | CNN      |
| 2024 | **CLIP+fusion** | CVPR           | Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space | [Paper](https://arxiv.org/pdf/2402.16267v1)                  |                                                              | CLIP     |
| 2024 | **SHIP**        | CVPR           | Probing Synergistic High-Order Interaction in Infrared and Visible Image Fusion | [Paper]()                                                    | [Code]()                                                     | CNN      |
| 2024 | CrossFuse       |                | CrossFuse: A novel cross attention mechanism based infrared and visible image fusion approach |                                                              |                                                              |          |
| 2024 | **DDBF**        | CVPR           | Dispel Darkness for Better Fusion: A Controllable Visual Enhancer based on Cross-modal Conditional Adversarial Learning | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Dispel_Darkness_for_Better_Fusion_A_Controllable_Visual_Enhancer_based_CVPR_2024_paper.pdf) | [code](https://github.com/HaoZhang1018/DDBF)                 | CNN      |
| 2024 | **MRFS**        | CVPR           | MRFS: Mutually Reinforcing Image Fusion and Segmentation     | [paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MRFS_Mutually_Reinforcing_Image_Fusion_and_Segmentation_CVPR_2024_paper.pdf) | [code](https://github.com/HaoZhang1018/MRFS)                 |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |
|      |                 |                |                                                              |                                                              |                                                              |          |

### 2024 CVPR

- [Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers](https://openaccess.thecvf.com/content/CVPR2024/html/Lee_Multi-criteria_Token_Fusion_with_One-step-ahead_Attention_for_Efficient_Vision_Transformers_CVPR_2024_paper.html)

  [Sanghyeok Lee](https://openaccess.thecvf.com/CVPR2024#), [Joonmyung Choi](https://openaccess.thecvf.com/CVPR2024#), [Hyunwoo J. Kim](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Multi-criteria_Token_Fusion_with_One-step-ahead_Attention_for_Efficient_Vision_Transformers_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lee_Multi-criteria_Token_Fusion_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2403.10030)]  

-  [DMR: Decomposed Multi-Modality Representations for Frames and Events Fusion in Visual Reinforcement Learning](https://openaccess.thecvf.com/content/CVPR2024/html/Xu_DMR_Decomposed_Multi-Modality_Representations_for_Frames_and_Events_Fusion_in_CVPR_2024_paper.html)

  [Haoran Xu](https://openaccess.thecvf.com/CVPR2024#), [Peixi Peng](https://openaccess.thecvf.com/CVPR2024#), [Guang Tan](https://openaccess.thecvf.com/CVPR2024#), [Yuan Li](https://openaccess.thecvf.com/CVPR2024#), [Xinhai Xu](https://openaccess.thecvf.com/CVPR2024#), [Yonghong Tian](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_DMR_Decomposed_Multi-Modality_Representations_for_Frames_and_Events_Fusion_in_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Xu_DMR_Decomposed_Multi-Modality_CVPR_2024_supplemental.pdf)]  

-  [Task-Customized Mixture of Adapters for General Image Fusion](https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_Task-Customized_Mixture_of_Adapters_for_General_Image_Fusion_CVPR_2024_paper.html)

  [Pengfei Zhu](https://openaccess.thecvf.com/CVPR2024#), [Yang Sun](https://openaccess.thecvf.com/CVPR2024#), [Bing Cao](https://openaccess.thecvf.com/CVPR2024#), [Qinghua Hu](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Task-Customized_Mixture_of_Adapters_for_General_Image_Fusion_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhu_Task-Customized_Mixture_of_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2403.12494)]  

-  [Bi-SSC: Geometric-Semantic Bidirectional Fusion for Camera-based 3D Semantic Scene Completion](https://openaccess.thecvf.com/content/CVPR2024/html/Xue_Bi-SSC_Geometric-Semantic_Bidirectional_Fusion_for_Camera-based_3D_Semantic_Scene_Completion_CVPR_2024_paper.html)

  [Yujie Xue](https://openaccess.thecvf.com/CVPR2024#), [Ruihui Li](https://openaccess.thecvf.com/CVPR2024#), [Fan Wu](https://openaccess.thecvf.com/CVPR2024#), [Zhuo Tang](https://openaccess.thecvf.com/CVPR2024#), [Kenli Li](https://openaccess.thecvf.com/CVPR2024#), [Mingxing Duan](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Xue_Bi-SSC_Geometric-Semantic_Bidirectional_Fusion_for_Camera-based_3D_Semantic_Scene_Completion_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Xue_Bi-SSC_Geometric-Semantic_Bidirectional_CVPR_2024_supplemental.pdf)]  

-  [Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion](https://openaccess.thecvf.com/content/CVPR2024/html/Ai_Elite360D_Towards_Efficient_360_Depth_Estimation_via_Semantic-_and_Distance-Aware_CVPR_2024_paper.html)

  [Hao Ai](https://openaccess.thecvf.com/CVPR2024#), [Lin Wang](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Ai_Elite360D_Towards_Efficient_360_Depth_Estimation_via_Semantic-_and_Distance-Aware_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Ai_Elite360D_Towards_Efficient_CVPR_2024_supplemental.pdf)]  

-  [Contrastive Pre-Training with Multi-View Fusion for No-Reference Point Cloud Quality Assessment](https://openaccess.thecvf.com/content/CVPR2024/html/Shan_Contrastive_Pre-Training_with_Multi-View_Fusion_for_No-Reference_Point_Cloud_Quality_CVPR_2024_paper.html)

  [Ziyu Shan](https://openaccess.thecvf.com/CVPR2024#), [Yujie Zhang](https://openaccess.thecvf.com/CVPR2024#), [Qi Yang](https://openaccess.thecvf.com/CVPR2024#), [Haichen Yang](https://openaccess.thecvf.com/CVPR2024#), [Yiling Xu](https://openaccess.thecvf.com/CVPR2024#), [Jenq-Neng Hwang](https://openaccess.thecvf.com/CVPR2024#), [Xiaozhong Xu](https://openaccess.thecvf.com/CVPR2024#), [Shan Liu](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Shan_Contrastive_Pre-Training_with_Multi-View_Fusion_for_No-Reference_Point_Cloud_Quality_CVPR_2024_paper.pdf)] [[arXiv](http://arxiv.org/abs/2403.10066)]  

-  [Revisiting Spatial-Frequency Information Integration from a Hierarchical Perspective for Panchromatic and Multi-Spectral Image Fusion](https://openaccess.thecvf.com/content/CVPR2024/html/Tan_Revisiting_Spatial-Frequency_Information_Integration_from_a_Hierarchical_Perspective_for_Panchromatic_CVPR_2024_paper.html)

  [Jiangtong Tan](https://openaccess.thecvf.com/CVPR2024#), [Jie Huang](https://openaccess.thecvf.com/CVPR2024#), [Naishan Zheng](https://openaccess.thecvf.com/CVPR2024#), [Man Zhou](https://openaccess.thecvf.com/CVPR2024#), [Keyu Yan](https://openaccess.thecvf.com/CVPR2024#), [Danfeng Hong](https://openaccess.thecvf.com/CVPR2024#), [Feng Zhao](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Tan_Revisiting_Spatial-Frequency_Information_Integration_from_a_Hierarchical_Perspective_for_Panchromatic_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Tan_Revisiting_Spatial-Frequency_Information_CVPR_2024_supplemental.pdf)]  

-  [Probing Synergistic High-Order Interaction in Infrared and Visible Image Fusion](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Probing_Synergistic_High-Order_Interaction_in_Infrared_and_Visible_Image_Fusion_CVPR_2024_paper.html)

  [Naishan Zheng](https://openaccess.thecvf.com/CVPR2024#), [Man Zhou](https://openaccess.thecvf.com/CVPR2024#), [Jie Huang](https://openaccess.thecvf.com/CVPR2024#), [Junming Hou](https://openaccess.thecvf.com/CVPR2024#), [Haoying Li](https://openaccess.thecvf.com/CVPR2024#), [Yuan Xu](https://openaccess.thecvf.com/CVPR2024#), [Feng Zhao](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Probing_Synergistic_High-Order_Interaction_in_Infrared_and_Visible_Image_Fusion_CVPR_2024_paper.pdf)]  

-  [Puff-Net: Efficient Style Transfer with Pure Content and Style Feature Fusion Network](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Puff-Net_Efficient_Style_Transfer_with_Pure_Content_and_Style_Feature_CVPR_2024_paper.html)

  [Sizhe Zheng](https://openaccess.thecvf.com/CVPR2024#), [Pan Gao](https://openaccess.thecvf.com/CVPR2024#), [Peng Zhou](https://openaccess.thecvf.com/CVPR2024#), [Jie Qin](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Zheng_Puff-Net_Efficient_Style_Transfer_with_Pure_Content_and_Style_Feature_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zheng_Puff-Net_Efficient_Style_CVPR_2024_supplemental.pdf)]  

-  [AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection](https://openaccess.thecvf.com/content/CVPR2024/html/Oorloff_AVFF_Audio-Visual_Feature_Fusion_for_Video_Deepfake_Detection_CVPR_2024_paper.html)

  [Trevine Oorloff](https://openaccess.thecvf.com/CVPR2024#), [Surya Koppisetti](https://openaccess.thecvf.com/CVPR2024#), [Nicolò Bonettini](https://openaccess.thecvf.com/CVPR2024#), [Divyaraj Solanki](https://openaccess.thecvf.com/CVPR2024#), [Ben Colman](https://openaccess.thecvf.com/CVPR2024#), [Yaser Yacoob](https://openaccess.thecvf.com/CVPR2024#), [Ali Shahriyari](https://openaccess.thecvf.com/CVPR2024#), [Gaurav Bharaj](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Oorloff_AVFF_Audio-Visual_Feature_Fusion_for_Video_Deepfake_Detection_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Oorloff_AVFF_Audio-Visual_Feature_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2406.02951)]  

-  [Event-based Visible and Infrared Fusion via Multi-task Collaboration](https://openaccess.thecvf.com/content/CVPR2024/html/Geng_Event-based_Visible_and_Infrared_Fusion_via_Multi-task_Collaboration_CVPR_2024_paper.html)

  [Mengyue Geng](https://openaccess.thecvf.com/CVPR2024#), [Lin Zhu](https://openaccess.thecvf.com/CVPR2024#), [Lizhi Wang](https://openaccess.thecvf.com/CVPR2024#), [Wei Zhang](https://openaccess.thecvf.com/CVPR2024#), [Ruiqin Xiong](https://openaccess.thecvf.com/CVPR2024#), [Yonghong Tian](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Geng_Event-based_Visible_and_Infrared_Fusion_via_Multi-task_Collaboration_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Geng_Event-based_Visible_and_CVPR_2024_supplemental.pdf)]  

-  [PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Lv_PLACE_Adaptive_Layout-Semantic_Fusion_for_Semantic_Image_Synthesis_CVPR_2024_paper.html)

  [Zhengyao Lv](https://openaccess.thecvf.com/CVPR2024#), [Yuxiang Wei](https://openaccess.thecvf.com/CVPR2024#), [Wangmeng Zuo](https://openaccess.thecvf.com/CVPR2024#), [Kwan-Yee K. Wong](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Lv_PLACE_Adaptive_Layout-Semantic_Fusion_for_Semantic_Image_Synthesis_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lv_PLACE_Adaptive_Layout-Semantic_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2403.01852)]  

-  [Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling](https://openaccess.thecvf.com/content/CVPR2024/html/Mo_Unveiling_the_Power_of_Audio-Visual_Early_Fusion_Transformers_with_Dense_CVPR_2024_paper.html)

  [Shentong Mo](https://openaccess.thecvf.com/CVPR2024#), [Pedro Morgado](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_Unveiling_the_Power_of_Audio-Visual_Early_Fusion_Transformers_with_Dense_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Mo_Unveiling_the_Power_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2312.01017)] 

-  [MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Hu_MVD-Fusion_Single-view_3D_via_Depth-consistent_Multi-view_Generation_CVPR_2024_paper.html)

  [Hanzhe Hu](https://openaccess.thecvf.com/CVPR2024#), [Zhizhuo Zhou](https://openaccess.thecvf.com/CVPR2024#), [Varun Jampani](https://openaccess.thecvf.com/CVPR2024#), [Shubham Tulsiani](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_MVD-Fusion_Single-view_3D_via_Depth-consistent_Multi-view_Generation_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Hu_MVD-Fusion_Single-view_3D_CVPR_2024_supplemental.pdf)]  

-  [RCBEVDet: Radar-camera Fusion in Bird's Eye View for 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2024/html/Lin_RCBEVDet_Radar-camera_Fusion_in_Birds_Eye_View_for_3D_Object_CVPR_2024_paper.html)

  [Zhiwei Lin](https://openaccess.thecvf.com/CVPR2024#), [Zhe Liu](https://openaccess.thecvf.com/CVPR2024#), [Zhongyu Xia](https://openaccess.thecvf.com/CVPR2024#), [Xinhao Wang](https://openaccess.thecvf.com/CVPR2024#), [Yongtao Wang](https://openaccess.thecvf.com/CVPR2024#), [Shengxiang Qi](https://openaccess.thecvf.com/CVPR2024#), [Yang Dong](https://openaccess.thecvf.com/CVPR2024#), [Nan Dong](https://openaccess.thecvf.com/CVPR2024#), [Le Zhang](https://openaccess.thecvf.com/CVPR2024#), [Ce Zhu](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_RCBEVDet_Radar-camera_Fusion_in_Birds_Eye_View_for_3D_Object_CVPR_2024_paper.pdf)]  

-  [Data-Efficient Multimodal Fusion on a Single GPU](https://openaccess.thecvf.com/content/CVPR2024/html/Vouitsis_Data-Efficient_Multimodal_Fusion_on_a_Single_GPU_CVPR_2024_paper.html)

  [Noël Vouitsis](https://openaccess.thecvf.com/CVPR2024#), [Zhaoyan Liu](https://openaccess.thecvf.com/CVPR2024#), [Satya Krishna Gorti](https://openaccess.thecvf.com/CVPR2024#), [Valentin Villecroze](https://openaccess.thecvf.com/CVPR2024#), [Jesse C. Cresswell](https://openaccess.thecvf.com/CVPR2024#), [Guangwei Yu](https://openaccess.thecvf.com/CVPR2024#), [Gabriel Loaiza-Ganem](https://openaccess.thecvf.com/CVPR2024#), [Maksims Volkovs](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Vouitsis_Data-Efficient_Multimodal_Fusion_on_a_Single_GPU_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Vouitsis_Data-Efficient_Multimodal_Fusion_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2312.10144)]  

-  [ColorPCR: Color Point Cloud Registration with Multi-Stage Geometric-Color Fusion](https://openaccess.thecvf.com/content/CVPR2024/html/Mu_ColorPCR_Color_Point_Cloud_Registration_with_Multi-Stage_Geometric-Color_Fusion_CVPR_2024_paper.html)

  [Juncheng Mu](https://openaccess.thecvf.com/CVPR2024#), [Lin Bie](https://openaccess.thecvf.com/CVPR2024#), [Shaoyi Du](https://openaccess.thecvf.com/CVPR2024#), [Yue Gao](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Mu_ColorPCR_Color_Point_Cloud_Registration_with_Multi-Stage_Geometric-Color_Fusion_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Mu_ColorPCR_Color_Point_CVPR_2024_supplemental.pdf)]  

-  [Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Bring_Event_into_RGB_and_LiDAR_Hierarchical_Visual-Motion_Fusion_for_CVPR_2024_paper.html)

  [Hanyu Zhou](https://openaccess.thecvf.com/CVPR2024#), [Yi Chang](https://openaccess.thecvf.com/CVPR2024#), [Zhiwei Shi](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Bring_Event_into_RGB_and_LiDAR_Hierarchical_Visual-Motion_Fusion_for_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhou_Bring_Event_into_CVPR_2024_supplemental.zip)] [[arXiv](http://arxiv.org/abs/2403.07432)]  

-  [CPP-Net: Embracing Multi-Scale Feature Fusion into Deep Unfolding CP-PPA Network for Compressive Sensing](https://openaccess.thecvf.com/content/CVPR2024/html/Guo_CPP-Net_Embracing_Multi-Scale_Feature_Fusion_into_Deep_Unfolding_CP-PPA_Network_CVPR_2024_paper.html)

  [Zhen Guo](https://openaccess.thecvf.com/CVPR2024#), [Hongping Gan](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_CPP-Net_Embracing_Multi-Scale_Feature_Fusion_into_Deep_Unfolding_CP-PPA_Network_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Guo_CPP-Net_Embracing_Multi-Scale_CVPR_2024_supplemental.pdf)]  

-  [Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models](https://openaccess.thecvf.com/content/CVPR2024/html/Kwon_Concept_Weaver_Enabling_Multi-Concept_Fusion_in_Text-to-Image_Models_CVPR_2024_paper.html)

  [Gihyun Kwon](https://openaccess.thecvf.com/CVPR2024#), [Simon Jenni](https://openaccess.thecvf.com/CVPR2024#), [Dingzeyu Li](https://openaccess.thecvf.com/CVPR2024#), [Joon-Young Lee](https://openaccess.thecvf.com/CVPR2024#), [Jong Chul Ye](https://openaccess.thecvf.com/CVPR2024#), [Fabian Caba Heilbron](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Kwon_Concept_Weaver_Enabling_Multi-Concept_Fusion_in_Text-to-Image_Models_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Kwon_Concept_Weaver_Enabling_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2404.03913)]  

-  [SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_SecondPose_SE3-Consistent_Dual-Stream_Feature_Fusion_for_Category-Level_Pose_Estimation_CVPR_2024_paper.html)

  [Yamei Chen](https://openaccess.thecvf.com/CVPR2024#), [Yan Di](https://openaccess.thecvf.com/CVPR2024#), [Guangyao Zhai](https://openaccess.thecvf.com/CVPR2024#), [Fabian Manhardt](https://openaccess.thecvf.com/CVPR2024#), [Chenyangguang Zhang](https://openaccess.thecvf.com/CVPR2024#), [Ruida Zhang](https://openaccess.thecvf.com/CVPR2024#), [Federico Tombari](https://openaccess.thecvf.com/CVPR2024#), [Nassir Navab](https://openaccess.thecvf.com/CVPR2024#), [Benjamin Busam](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SecondPose_SE3-Consistent_Dual-Stream_Feature_Fusion_for_Category-Level_Pose_Estimation_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Chen_SecondPose_SE3-Consistent_Dual-Stream_CVPR_2024_supplemental.pdf)]  

-  [SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers](https://openaccess.thecvf.com/content/CVPR2024/html/Carter_SleepVST_Sleep_Staging_from_Near-Infrared_Video_Signals_using_Pre-Trained_Transformers_CVPR_2024_paper.html)

  [Jonathan F. Carter](https://openaccess.thecvf.com/CVPR2024#), [João Jorge](https://openaccess.thecvf.com/CVPR2024#), [Oliver Gibson](https://openaccess.thecvf.com/CVPR2024#), [Lionel Tarassenko](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Carter_SleepVST_Sleep_Staging_from_Near-Infrared_Video_Signals_using_Pre-Trained_Transformers_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Carter_SleepVST_Sleep_Staging_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2404.03831)]  

-  [Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion](https://openaccess.thecvf.com/content/CVPR2024/html/Yi_Text-IF_Leveraging_Semantic_Text_Guidance_for_Degradation-Aware_and_Interactive_Image_CVPR_2024_paper.html)

  [Xunpeng Yi](https://openaccess.thecvf.com/CVPR2024#), [Han Xu](https://openaccess.thecvf.com/CVPR2024#), [Hao Zhang](https://openaccess.thecvf.com/CVPR2024#), [Linfeng Tang](https://openaccess.thecvf.com/CVPR2024#), [Jiayi Ma](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Yi_Text-IF_Leveraging_Semantic_Text_Guidance_for_Degradation-Aware_and_Interactive_Image_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Yi_Text-IF_Leveraging_Semantic_CVPR_2024_supplemental.pdf)]  

-  [Infrared Adversarial Car Stickers](https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_Infrared_Adversarial_Car_Stickers_CVPR_2024_paper.html)

  [Xiaopei Zhu](https://openaccess.thecvf.com/CVPR2024#), [Yuqiu Liu](https://openaccess.thecvf.com/CVPR2024#), [Zhanhao Hu](https://openaccess.thecvf.com/CVPR2024#), [Jianmin Li](https://openaccess.thecvf.com/CVPR2024#), [Xiaolin Hu](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Infrared_Adversarial_Car_Stickers_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhu_Infrared_Adversarial_Car_CVPR_2024_supplemental.zip)] [[arXiv](http://arxiv.org/abs/2405.09924)]  

-  [F3Loc: Fusion and Filtering for Floorplan Localization](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_F3Loc_Fusion_and_Filtering_for_Floorplan_Localization_CVPR_2024_paper.html)

  [Changan Chen](https://openaccess.thecvf.com/CVPR2024#), [Rui Wang](https://openaccess.thecvf.com/CVPR2024#), [Christoph Vogel](https://openaccess.thecvf.com/CVPR2024#), [Marc Pollefeys](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_F3Loc_Fusion_and_Filtering_for_Floorplan_Localization_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Chen_F3Loc_Fusion_and_CVPR_2024_supplemental.pdf)]  

-  [SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks](https://openaccess.thecvf.com/content/CVPR2024/html/Xie_SG-PGM_Partial_Graph_Matching_Network_with_Semantic_Geometric_Fusion_for_CVPR_2024_paper.html)

  [Yaxu Xie](https://openaccess.thecvf.com/CVPR2024#), [Alain Pagani](https://openaccess.thecvf.com/CVPR2024#), [Didier Stricker](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Xie_SG-PGM_Partial_Graph_Matching_Network_with_Semantic_Geometric_Fusion_for_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Xie_SG-PGM_Partial_Graph_CVPR_2024_supplemental.pdf)]  

-  [Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicles](https://openaccess.thecvf.com/content/CVPR2024/html/Song_Collaborative_Semantic_Occupancy_Prediction_with_Hybrid_Feature_Fusion_in_Connected_CVPR_2024_paper.html)

  [Rui Song](https://openaccess.thecvf.com/CVPR2024#), [Chenwei Liang](https://openaccess.thecvf.com/CVPR2024#), [Hu Cao](https://openaccess.thecvf.com/CVPR2024#), [Zhiran Yan](https://openaccess.thecvf.com/CVPR2024#), [Walter Zimmer](https://openaccess.thecvf.com/CVPR2024#), [Markus Gross](https://openaccess.thecvf.com/CVPR2024#), [Andreas Festag](https://openaccess.thecvf.com/CVPR2024#), [Alois Knoll](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Song_Collaborative_Semantic_Occupancy_Prediction_with_Hybrid_Feature_Fusion_in_Connected_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Song_Collaborative_Semantic_Occupancy_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2402.07635)]  

-  [Towards Robust 3D Object Detection with LiDAR and 4D Radar Fusion in Various Weather Conditions](https://openaccess.thecvf.com/content/CVPR2024/html/Chae_Towards_Robust_3D_Object_Detection_with_LiDAR_and_4D_Radar_CVPR_2024_paper.html)

  [Yujeong Chae](https://openaccess.thecvf.com/CVPR2024#), [Hyeonseong Kim](https://openaccess.thecvf.com/CVPR2024#), [Kuk-Jin Yoon](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Chae_Towards_Robust_3D_Object_Detection_with_LiDAR_and_4D_Radar_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Chae_Towards_Robust_3D_CVPR_2024_supplemental.pdf)]  

-  [Embracing Unimodal Aleatoric Uncertainty for Robust Multimodal Fusion](https://openaccess.thecvf.com/content/CVPR2024/html/Gao_Embracing_Unimodal_Aleatoric_Uncertainty_for_Robust_Multimodal_Fusion_CVPR_2024_paper.html)

  [Zixian Gao](https://openaccess.thecvf.com/CVPR2024#), [Xun Jiang](https://openaccess.thecvf.com/CVPR2024#), [Xing Xu](https://openaccess.thecvf.com/CVPR2024#), [Fumin Shen](https://openaccess.thecvf.com/CVPR2024#), [Yujie Li](https://openaccess.thecvf.com/CVPR2024#), [Heng Tao Shen](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_Embracing_Unimodal_Aleatoric_Uncertainty_for_Robust_Multimodal_Fusion_CVPR_2024_paper.pdf)]  

-  [IBD-SLAM: Learning Image-Based Depth Fusion for Generalizable SLAM](https://openaccess.thecvf.com/content/CVPR2024/html/Yin_IBD-SLAM_Learning_Image-Based_Depth_Fusion_for_Generalizable_SLAM_CVPR_2024_paper.html)

  [Minghao Yin](https://openaccess.thecvf.com/CVPR2024#), [Shangzhe Wu](https://openaccess.thecvf.com/CVPR2024#), [Kai Han](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_IBD-SLAM_Learning_Image-Based_Depth_Fusion_for_Generalizable_SLAM_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Yin_IBD-SLAM_Learning_Image-Based_CVPR_2024_supplemental.pdf)]  

-  [Unmixing Before Fusion: A Generalized Paradigm for Multi-Source-based Hyperspectral Image Synthesis](https://openaccess.thecvf.com/content/CVPR2024/html/Yu_Unmixing_Before_Fusion_A_Generalized_Paradigm_for_Multi-Source-based_Hyperspectral_Image_CVPR_2024_paper.html)

  [Yang Yu](https://openaccess.thecvf.com/CVPR2024#), [Erting Pan](https://openaccess.thecvf.com/CVPR2024#), [Xinya Wang](https://openaccess.thecvf.com/CVPR2024#), [Yuheng Wu](https://openaccess.thecvf.com/CVPR2024#), [Xiaoguang Mei](https://openaccess.thecvf.com/CVPR2024#), [Jiayi Ma](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Yu_Unmixing_Before_Fusion_A_Generalized_Paradigm_for_Multi-Source-based_Hyperspectral_Image_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Yu_Unmixing_Before_Fusion_CVPR_2024_supplemental.pdf)]  

-  [HINTED: Hard Instance Enhanced Detector with Mixed-Density Feature Fusion for Sparsely-Supervised 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2024/html/Xia_HINTED_Hard_Instance_Enhanced_Detector_with_Mixed-Density_Feature_Fusion_for_CVPR_2024_paper.html)

  [Qiming Xia](https://openaccess.thecvf.com/CVPR2024#), [Wei Ye](https://openaccess.thecvf.com/CVPR2024#), [Hai Wu](https://openaccess.thecvf.com/CVPR2024#), [Shijia Zhao](https://openaccess.thecvf.com/CVPR2024#), [Leyuan Xing](https://openaccess.thecvf.com/CVPR2024#), [Xun Huang](https://openaccess.thecvf.com/CVPR2024#), [Jinhao Deng](https://openaccess.thecvf.com/CVPR2024#), [Xin Li](https://openaccess.thecvf.com/CVPR2024#), [Chenglu Wen](https://openaccess.thecvf.com/CVPR2024#), [Cheng Wang](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Xia_HINTED_Hard_Instance_Enhanced_Detector_with_Mixed-Density_Feature_Fusion_for_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Xia_HINTED_Hard_Instance_CVPR_2024_supplemental.pdf)]  

-  [Neural Spline Fields for Burst Image Fusion and Layer Separation](https://openaccess.thecvf.com/content/CVPR2024/html/Chugunov_Neural_Spline_Fields_for_Burst_Image_Fusion_and_Layer_Separation_CVPR_2024_paper.html)

  [Ilya Chugunov](https://openaccess.thecvf.com/CVPR2024#), [David Shustin](https://openaccess.thecvf.com/CVPR2024#), [Ruyu Yan](https://openaccess.thecvf.com/CVPR2024#), [Chenyang Lei](https://openaccess.thecvf.com/CVPR2024#), [Felix Heide](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Chugunov_Neural_Spline_Fields_for_Burst_Image_Fusion_and_Layer_Separation_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Chugunov_Neural_Spline_Fields_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2312.14235)]  

-  [Equivariant Multi-Modality Image Fusion](https://openaccess.thecvf.com/content/CVPR2024/html/Zhao_Equivariant_Multi-Modality_Image_Fusion_CVPR_2024_paper.html)

  [Zixiang Zhao](https://openaccess.thecvf.com/CVPR2024#), [Haowen Bai](https://openaccess.thecvf.com/CVPR2024#), [Jiangshe Zhang](https://openaccess.thecvf.com/CVPR2024#), [Yulun Zhang](https://openaccess.thecvf.com/CVPR2024#), [Kai Zhang](https://openaccess.thecvf.com/CVPR2024#), [Shuang Xu](https://openaccess.thecvf.com/CVPR2024#), [Dongdong Chen](https://openaccess.thecvf.com/CVPR2024#), [Radu Timofte](https://openaccess.thecvf.com/CVPR2024#), [Luc Van Gool](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Equivariant_Multi-Modality_Image_Fusion_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhao_Equivariant_Multi-Modality_Image_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2305.11443)]  

-  [Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Geometry-aware_Reconstruction_and_Fusion-refined_Rendering_for_Generalizable_Neural_Radiance_Fields_CVPR_2024_paper.html)

  [Tianqi Liu](https://openaccess.thecvf.com/CVPR2024#), [Xinyi Ye](https://openaccess.thecvf.com/CVPR2024#), [Min Shi](https://openaccess.thecvf.com/CVPR2024#), [Zihao Huang](https://openaccess.thecvf.com/CVPR2024#), [Zhiyu Pan](https://openaccess.thecvf.com/CVPR2024#), [Zhan Peng](https://openaccess.thecvf.com/CVPR2024#), [Zhiguo Cao](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Geometry-aware_Reconstruction_and_Fusion-refined_Rendering_for_Generalizable_Neural_Radiance_Fields_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Liu_Geometry-aware_Reconstruction_and_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2404.17528)]  

-  [Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion](https://openaccess.thecvf.com/content/CVPR2024/html/Casarin_Your_Image_is_My_Video_Reshaping_the_Receptive_Field_via_CVPR_2024_paper.html)

  [Sofia Casarin](https://openaccess.thecvf.com/CVPR2024#), [Cynthia I. Ugwu](https://openaccess.thecvf.com/CVPR2024#), [Sergio Escalera](https://openaccess.thecvf.com/CVPR2024#), [Oswald Lanz](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Casarin_Your_Image_is_My_Video_Reshaping_the_Receptive_Field_via_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Casarin_Your_Image_is_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2403.15194)]  

-  [3D Multi-frame Fusion for Video Stabilization](https://openaccess.thecvf.com/content/CVPR2024/html/Peng_3D_Multi-frame_Fusion_for_Video_Stabilization_CVPR_2024_paper.html)

  [Zhan Peng](https://openaccess.thecvf.com/CVPR2024#), [Xinyi Ye](https://openaccess.thecvf.com/CVPR2024#), [Weiyue Zhao](https://openaccess.thecvf.com/CVPR2024#), [Tianqi Liu](https://openaccess.thecvf.com/CVPR2024#), [Huiqiang Sun](https://openaccess.thecvf.com/CVPR2024#), [Baopu Li](https://openaccess.thecvf.com/CVPR2024#), [Zhiguo Cao](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_3D_Multi-frame_Fusion_for_Video_Stabilization_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Peng_3D_Multi-frame_Fusion_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2404.12887)]  

-  [Neural Exposure Fusion for High-Dynamic Range Object Detection](https://openaccess.thecvf.com/content/CVPR2024/html/Onzon_Neural_Exposure_Fusion_for_High-Dynamic_Range_Object_Detection_CVPR_2024_paper.html)

  [Emmanuel Onzon](https://openaccess.thecvf.com/CVPR2024#), [Maximilian Bömer](https://openaccess.thecvf.com/CVPR2024#), [Fahim Mannan](https://openaccess.thecvf.com/CVPR2024#), [Felix Heide](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Onzon_Neural_Exposure_Fusion_for_High-Dynamic_Range_Object_Detection_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Onzon_Neural_Exposure_Fusion_CVPR_2024_supplemental.pdf)]  

-  [SFOD: Spiking Fusion Object Detector](https://openaccess.thecvf.com/content/CVPR2024/html/Fan_SFOD_Spiking_Fusion_Object_Detector_CVPR_2024_paper.html)

  [Yimeng Fan](https://openaccess.thecvf.com/CVPR2024#), [Wei Zhang](https://openaccess.thecvf.com/CVPR2024#), [Changsong Liu](https://openaccess.thecvf.com/CVPR2024#), [Mingyang Li](https://openaccess.thecvf.com/CVPR2024#), [Wenrui Lu](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_SFOD_Spiking_Fusion_Object_Detector_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Fan_SFOD_Spiking_Fusion_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2403.15192)]  

-  [Dispel Darkness for Better Fusion: A Controllable Visual Enhancer based on Cross-modal Conditional Adversarial Learning](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Dispel_Darkness_for_Better_Fusion_A_Controllable_Visual_Enhancer_based_CVPR_2024_paper.html)

  [Hao Zhang](https://openaccess.thecvf.com/CVPR2024#), [Linfeng Tang](https://openaccess.thecvf.com/CVPR2024#), [Xinyu Xiang](https://openaccess.thecvf.com/CVPR2024#), [Xuhui Zuo](https://openaccess.thecvf.com/CVPR2024#), [Jiayi Ma](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Dispel_Darkness_for_Better_Fusion_A_Controllable_Visual_Enhancer_based_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhang_Dispel_Darkness_for_CVPR_2024_supplemental.pdf)]  

-  [MRFS: Mutually Reinforcing Image Fusion and Segmentation](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_MRFS_Mutually_Reinforcing_Image_Fusion_and_Segmentation_CVPR_2024_paper.html)

  [Hao Zhang](https://openaccess.thecvf.com/CVPR2024#), [Xuhui Zuo](https://openaccess.thecvf.com/CVPR2024#), [Jie Jiang](https://openaccess.thecvf.com/CVPR2024#), [Chunchao Guo](https://openaccess.thecvf.com/CVPR2024#), [Jiayi Ma](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MRFS_Mutually_Reinforcing_Image_Fusion_and_Segmentation_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhang_MRFS_Mutually_Reinforcing_CVPR_2024_supplemental.pdf)]  

-  [Robust Depth Enhancement via Polarization Prompt Fusion Tuning](https://openaccess.thecvf.com/content/CVPR2024/html/Ikemura_Robust_Depth_Enhancement_via_Polarization_Prompt_Fusion_Tuning_CVPR_2024_paper.html)

  [Kei Ikemura](https://openaccess.thecvf.com/CVPR2024#), [Yiming Huang](https://openaccess.thecvf.com/CVPR2024#), [Felix Heide](https://openaccess.thecvf.com/CVPR2024#), [Zhaoxiang Zhang](https://openaccess.thecvf.com/CVPR2024#), [Qifeng Chen](https://openaccess.thecvf.com/CVPR2024#), [Chenyang Lei](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Ikemura_Robust_Depth_Enhancement_via_Polarization_Prompt_Fusion_Tuning_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Ikemura_Robust_Depth_Enhancement_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2404.04318)]  

-  [SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2024/html/Ye_SG-BEV_Satellite-Guided_BEV_Fusion_for_Cross-View_Semantic_Segmentation_CVPR_2024_paper.html)

  [Junyan Ye](https://openaccess.thecvf.com/CVPR2024#), [Qiyan Luo](https://openaccess.thecvf.com/CVPR2024#), [Jinhua Yu](https://openaccess.thecvf.com/CVPR2024#), [Huaping Zhong](https://openaccess.thecvf.com/CVPR2024#), [Zhimeng Zheng](https://openaccess.thecvf.com/CVPR2024#), [Conghui He](https://openaccess.thecvf.com/CVPR2024#), [Weijia Li](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Ye_SG-BEV_Satellite-Guided_BEV_Fusion_for_Cross-View_Semantic_Segmentation_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Ye_SG-BEV_Satellite-Guided_BEV_CVPR_2024_supplemental.pdf)]  

-  [Shallow-Deep Collaborative Learning for Unsupervised Visible-Infrared Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Shallow-Deep_Collaborative_Learning_for_Unsupervised_Visible-Infrared_Person_Re-Identification_CVPR_2024_paper.html)

  [Bin Yang](https://openaccess.thecvf.com/CVPR2024#), [Jun Chen](https://openaccess.thecvf.com/CVPR2024#), [Mang Ye](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Shallow-Deep_Collaborative_Learning_for_Unsupervised_Visible-Infrared_Person_Re-Identification_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Yang_Shallow-Deep_Collaborative_Learning_CVPR_2024_supplemental.pdf)]  

-  [Exact Fusion via Feature Distribution Matching for Few-shot Image Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Exact_Fusion_via_Feature_Distribution_Matching_for_Few-shot_Image_Generation_CVPR_2024_paper.html)

  [Yingbo Zhou](https://openaccess.thecvf.com/CVPR2024#), [Yutong Ye](https://openaccess.thecvf.com/CVPR2024#), [Pengyu Zhang](https://openaccess.thecvf.com/CVPR2024#), [Xian Wei](https://openaccess.thecvf.com/CVPR2024#), [Mingsong Chen](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Exact_Fusion_via_Feature_Distribution_Matching_for_Few-shot_Image_Generation_CVPR_2024_paper.pdf)]  

-  [Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2024/html/Ren_Implicit_Discriminative_Knowledge_Learning_for_Visible-Infrared_Person_Re-Identification_CVPR_2024_paper.html)

  [Kaijie Ren](https://openaccess.thecvf.com/CVPR2024#), [Lei Zhang](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_Implicit_Discriminative_Knowledge_Learning_for_Visible-Infrared_Person_Re-Identification_CVPR_2024_paper.pdf)] [[arXiv](http://arxiv.org/abs/2403.11708)]  

-  [Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_Adaptive_Fusion_of_Single-View_and_Multi-View_Depth_for_Autonomous_Driving_CVPR_2024_paper.html)

  [Junda Cheng](https://openaccess.thecvf.com/CVPR2024#), [Wei Yin](https://openaccess.thecvf.com/CVPR2024#), [Kaixuan Wang](https://openaccess.thecvf.com/CVPR2024#), [Xiaozhi Chen](https://openaccess.thecvf.com/CVPR2024#), [Shijie Wang](https://openaccess.thecvf.com/CVPR2024#), [Xin Yang](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_Adaptive_Fusion_of_Single-View_and_Multi-View_Depth_for_Autonomous_Driving_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Cheng_Adaptive_Fusion_of_CVPR_2024_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2403.07535)]  

-  [Infrared Small Target Detection with Scale and Location Sensitivity](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Infrared_Small_Target_Detection_with_Scale_and_Location_Sensitivity_CVPR_2024_paper.html)

  [Qiankun Liu](https://openaccess.thecvf.com/CVPR2024#), [Rui Liu](https://openaccess.thecvf.com/CVPR2024#), [Bolun Zheng](https://openaccess.thecvf.com/CVPR2024#), [Hongkui Wang](https://openaccess.thecvf.com/CVPR2024#), [Ying Fu](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Infrared_Small_Target_Detection_with_Scale_and_Location_Sensitivity_CVPR_2024_paper.pdf)] [[arXiv](http://arxiv.org/abs/2403.19366)]  

-  [IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2024/html/Yin_IS-Fusion_Instance-Scene_Collaborative_Fusion_for_Multimodal_3D_Object_Detection_CVPR_2024_paper.html)

  [Junbo Yin](https://openaccess.thecvf.com/CVPR2024#), [Jianbing Shen](https://openaccess.thecvf.com/CVPR2024#), [Runnan Chen](https://openaccess.thecvf.com/CVPR2024#), [Wei Li](https://openaccess.thecvf.com/CVPR2024#), [Ruigang Yang](https://openaccess.thecvf.com/CVPR2024#), [Pascal Frossard](https://openaccess.thecvf.com/CVPR2024#), [Wenguan Wang](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_IS-Fusion_Instance-Scene_Collaborative_Fusion_for_Multimodal_3D_Object_Detection_CVPR_2024_paper.pdf)]  

-  [Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates](https://openaccess.thecvf.com/content/CVPR2024/html/Shum_Language-driven_Object_Fusion_into_Neural_Radiance_Fields_with_Pose-Conditioned_Dataset_CVPR_2024_paper.html)

  [Ka Chun Shum](https://openaccess.thecvf.com/CVPR2024#), [Jaeyeon Kim](https://openaccess.thecvf.com/CVPR2024#), [Binh-Son Hua](https://openaccess.thecvf.com/CVPR2024#), [Duc Thanh Nguyen](https://openaccess.thecvf.com/CVPR2024#), [Sai-Kit Yeung](https://openaccess.thecvf.com/CVPR2024#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Shum_Language-driven_Object_Fusion_into_Neural_Radiance_Fields_with_Pose-Conditioned_Dataset_CVPR_2024_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Shum_Language-driven_Object_Fusion_CVPR_2024_supplemental.zip)] [[arXiv](http://arxiv.org/abs/2309.11281)]  

### 2025 CVPR

-  [Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices](https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Multi-Layer_Visual_Feature_Fusion_in_Multimodal_LLMs_Methods_Analysis_and_CVPR_2025_paper.html)

  [Junyan Lin](https://openaccess.thecvf.com/CVPR2025#), [Haoran Chen](https://openaccess.thecvf.com/CVPR2025#), [Yue Fan](https://openaccess.thecvf.com/CVPR2025#), [Yingqi Fan](https://openaccess.thecvf.com/CVPR2025#), [Xin Jin](https://openaccess.thecvf.com/CVPR2025#), [Hui Su](https://openaccess.thecvf.com/CVPR2025#), [Jinlan Fu](https://openaccess.thecvf.com/CVPR2025#), [Xiaoyu Shen](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_Multi-Layer_Visual_Feature_Fusion_in_Multimodal_LLMs_Methods_Analysis_and_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Lin_Multi-Layer_Visual_Feature_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2503.06063)]  

-  [TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion_CVPR_2025_paper.html)

  [Yiran Wang](https://openaccess.thecvf.com/CVPR2025#), [Jiaqi Li](https://openaccess.thecvf.com/CVPR2025#), [Chaoyi Hong](https://openaccess.thecvf.com/CVPR2025#), [Ruibo Li](https://openaccess.thecvf.com/CVPR2025#), [Liusheng Sun](https://openaccess.thecvf.com/CVPR2025#), [Xiao Song](https://openaccess.thecvf.com/CVPR2025#), [Zhe Wang](https://openaccess.thecvf.com/CVPR2025#), [Zhiguo Cao](https://openaccess.thecvf.com/CVPR2025#), [Guosheng Lin](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_TacoDepth_Towards_Efficient_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2504.11773)]  

-  [SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models](https://openaccess.thecvf.com/content/CVPR2025/html/Miller_SPARC_Score_Prompting_and_Adaptive_Fusion_for_Zero-Shot_Multi-Label_Recognition_CVPR_2025_paper.html)

  [Kevin Miller](https://openaccess.thecvf.com/CVPR2025#), [Aditya Gangrade](https://openaccess.thecvf.com/CVPR2025#), [Samarth Mishra](https://openaccess.thecvf.com/CVPR2025#), [Kate Saenko](https://openaccess.thecvf.com/CVPR2025#), [Venkatesh Saligrama](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Miller_SPARC_Score_Prompting_and_Adaptive_Fusion_for_Zero-Shot_Multi-Label_Recognition_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Miller_SPARC_Score_Prompting_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2502.16911)]  

-  [One Model for ALL: Low-Level Task Interaction Is a Key to Task-Agnostic Image Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_One_Model_for_ALL_Low-Level_Task_Interaction_Is_a_Key_CVPR_2025_paper.html)

  [Chunyang Cheng](https://openaccess.thecvf.com/CVPR2025#), [Tianyang Xu](https://openaccess.thecvf.com/CVPR2025#), [Zhenhua Feng](https://openaccess.thecvf.com/CVPR2025#), [Xiaojun Wu](https://openaccess.thecvf.com/CVPR2025#), [Zhangyong Tang](https://openaccess.thecvf.com/CVPR2025#), [Hui Li](https://openaccess.thecvf.com/CVPR2025#), [Zeyang Zhang](https://openaccess.thecvf.com/CVPR2025#), [Sara Atito](https://openaccess.thecvf.com/CVPR2025#), [Muhammad Awais](https://openaccess.thecvf.com/CVPR2025#), [Josef Kittler](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Cheng_One_Model_for_ALL_Low-Level_Task_Interaction_Is_a_Key_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Cheng_One_Model_for_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2502.19854)]  

-  [From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification](https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_From_Laboratory_to_Real_World_A_New_Benchmark_Towards_Privacy-Preserved_CVPR_2025_paper.html)

  [Yan Jiang](https://openaccess.thecvf.com/CVPR2025#), [Hao Yu](https://openaccess.thecvf.com/CVPR2025#), [Xu Cheng](https://openaccess.thecvf.com/CVPR2025#), [Haoyu Chen](https://openaccess.thecvf.com/CVPR2025#), [Zhaodong Sun](https://openaccess.thecvf.com/CVPR2025#), [Guoying Zhao](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Jiang_From_Laboratory_to_Real_World_A_New_Benchmark_Towards_Privacy-Preserved_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Jiang_From_Laboratory_to_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2503.12232)]  

-  [Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering](https://openaccess.thecvf.com/content/CVPR2025/html/Zou_Alignment_Mining_and_Fusion_Representation_Alignment_with_Hard_Negative_Mining_CVPR_2025_paper.html)

  [Yuanhao Zou](https://openaccess.thecvf.com/CVPR2025#), [Zhaozheng Yin](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Zou_Alignment_Mining_and_Fusion_Representation_Alignment_with_Hard_Negative_Mining_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Zou_Alignment_Mining_and_CVPR_2025_supplemental.pdf)]  

-  [DriveScape: High-Resolution Driving Video Generation by Multi-View Feature Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DriveScape_High-Resolution_Driving_Video_Generation_by_Multi-View_Feature_Fusion_CVPR_2025_paper.html)

  [Wei Wu](https://openaccess.thecvf.com/CVPR2025#), [Xi Guo](https://openaccess.thecvf.com/CVPR2025#), [Weixuan Tang](https://openaccess.thecvf.com/CVPR2025#), [Tingxuan Huang](https://openaccess.thecvf.com/CVPR2025#), [Chiyu Wang](https://openaccess.thecvf.com/CVPR2025#), [Chenjing Ding](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_DriveScape_High-Resolution_Driving_Video_Generation_by_Multi-View_Feature_Fusion_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wu_DriveScape_High-Resolution_Driving_CVPR_2025_supplemental.pdf)]  

-  [Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond](https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Every_SAM_Drop_Counts_Embracing_Semantic_Priors_for_Multi-Modality_Image_CVPR_2025_paper.html)

  [Guanyao Wu](https://openaccess.thecvf.com/CVPR2025#), [Haoyu Liu](https://openaccess.thecvf.com/CVPR2025#), [Hongming Fu](https://openaccess.thecvf.com/CVPR2025#), [Yichuan Peng](https://openaccess.thecvf.com/CVPR2025#), [Jinyuan Liu](https://openaccess.thecvf.com/CVPR2025#), [Xin Fan](https://openaccess.thecvf.com/CVPR2025#), [Risheng Liu](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_Every_SAM_Drop_Counts_Embracing_Semantic_Priors_for_Multi-Modality_Image_CVPR_2025_paper.pdf)] [[arXiv](http://arxiv.org/abs/2503.01210)]  

-  [A Selective Re-learning Mechanism for Hyperspectral Fusion Imaging](https://openaccess.thecvf.com/content/CVPR2025/html/Liu_A_Selective_Re-learning_Mechanism_for_Hyperspectral_Fusion_Imaging_CVPR_2025_paper.html)

  [Yuanye Liu](https://openaccess.thecvf.com/CVPR2025#), [Jinyang Liu](https://openaccess.thecvf.com/CVPR2025#), [Renwei Dian](https://openaccess.thecvf.com/CVPR2025#), [Shutao Li](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_A_Selective_Re-learning_Mechanism_for_Hyperspectral_Fusion_Imaging_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Liu_A_Selective_Re-learning_CVPR_2025_supplemental.pdf)]  

-  [HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Xu_HiFi-Portrait_Zero-shot_Identity-preserved_Portrait_Generation_with_High-fidelity_Multi-face_Fusion_CVPR_2025_paper.html)

  [Yifang Xu](https://openaccess.thecvf.com/CVPR2025#), [Benxiang Zhai](https://openaccess.thecvf.com/CVPR2025#), [Yunzhuo Sun](https://openaccess.thecvf.com/CVPR2025#), [Ming Li](https://openaccess.thecvf.com/CVPR2025#), [Yang Li](https://openaccess.thecvf.com/CVPR2025#), [Sidan Du](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_HiFi-Portrait_Zero-shot_Identity-preserved_Portrait_Generation_with_High-fidelity_Multi-face_Fusion_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Xu_HiFi-Portrait_Zero-shot_Identity-preserved_CVPR_2025_supplemental.pdf)]  

-  [K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs](https://openaccess.thecvf.com/content/CVPR2025/html/Ouyang_K-LoRA_Unlocking_Training-Free_Fusion_of_Any_Subject_and_Style_LoRAs_CVPR_2025_paper.html)

  [Ziheng Ouyang](https://openaccess.thecvf.com/CVPR2025#), [Zhen Li](https://openaccess.thecvf.com/CVPR2025#), [Qibin Hou](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Ouyang_K-LoRA_Unlocking_Training-Free_Fusion_of_Any_Subject_and_Style_LoRAs_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Ouyang_K-LoRA_Unlocking_Training-Free_CVPR_2025_supplemental.pdf)]  

-  [RaCFormer: Towards High-Quality 3D Object Detection via Query-based Radar-Camera Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Chu_RaCFormer_Towards_High-Quality_3D_Object_Detection_via_Query-based_Radar-Camera_Fusion_CVPR_2025_paper.html)

  [Xiaomeng Chu](https://openaccess.thecvf.com/CVPR2025#), [Jiajun Deng](https://openaccess.thecvf.com/CVPR2025#), [Guoliang You](https://openaccess.thecvf.com/CVPR2025#), [Yifan Duan](https://openaccess.thecvf.com/CVPR2025#), [Houqiang Li](https://openaccess.thecvf.com/CVPR2025#), [Yanyong Zhang](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Chu_RaCFormer_Towards_High-Quality_3D_Object_Detection_via_Query-based_Radar-Camera_Fusion_CVPR_2025_paper.pdf)] [[arXiv](http://arxiv.org/abs/2412.12725)]  

-  [Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Florence-VL_Enhancing_Vision-Language_Models_with_Generative_Vision_Encoder_and_Depth-Breadth_CVPR_2025_paper.html)

  [Jiuhai Chen](https://openaccess.thecvf.com/CVPR2025#), [Jianwei Yang](https://openaccess.thecvf.com/CVPR2025#), [Haiping Wu](https://openaccess.thecvf.com/CVPR2025#), [Dianqi Li](https://openaccess.thecvf.com/CVPR2025#), [Jianfeng Gao](https://openaccess.thecvf.com/CVPR2025#), [Tianyi Zhou](https://openaccess.thecvf.com/CVPR2025#), [Bin Xiao](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Florence-VL_Enhancing_Vision-Language_Models_with_Generative_Vision_Encoder_and_Depth-Breadth_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Chen_Florence-VL_Enhancing_Vision-Language_CVPR_2025_supplemental.pdf)]  

-  [DifIISR: A Diffusion Model with Gradient Guidance for Infrared Image Super-Resolution](https://openaccess.thecvf.com/content/CVPR2025/html/Li_DifIISR_A_Diffusion_Model_with_Gradient_Guidance_for_Infrared_Image_CVPR_2025_paper.html)

  [Xingyuan Li](https://openaccess.thecvf.com/CVPR2025#), [Zirui Wang](https://openaccess.thecvf.com/CVPR2025#), [Yang Zou](https://openaccess.thecvf.com/CVPR2025#), [Zhixin Chen](https://openaccess.thecvf.com/CVPR2025#), [Jun Ma](https://openaccess.thecvf.com/CVPR2025#), [Zhiying Jiang](https://openaccess.thecvf.com/CVPR2025#), [Long Ma](https://openaccess.thecvf.com/CVPR2025#), [Jinyuan Liu](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_DifIISR_A_Diffusion_Model_with_Gradient_Guidance_for_Infrared_Image_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Li_DifIISR_A_Diffusion_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2503.01187)]  

-  [Coherent 3D Portrait Video Reconstruction via Triplane Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Coherent_3D_Portrait_Video_Reconstruction_via_Triplane_Fusion_CVPR_2025_paper.html)

  [Shengze Wang](https://openaccess.thecvf.com/CVPR2025#), [Xueting Li](https://openaccess.thecvf.com/CVPR2025#), [Chao Liu](https://openaccess.thecvf.com/CVPR2025#), [Matthew Chan](https://openaccess.thecvf.com/CVPR2025#), [Michael Stengel](https://openaccess.thecvf.com/CVPR2025#), [Henry Fuchs](https://openaccess.thecvf.com/CVPR2025#), [Shalini De Mello](https://openaccess.thecvf.com/CVPR2025#), [Koki Nagano](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Coherent_3D_Portrait_Video_Reconstruction_via_Triplane_Fusion_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_Coherent_3D_Portrait_CVPR_2025_supplemental.zip)]  

-  [Binarized Neural Network for Multi-spectral Image Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Hou_Binarized_Neural_Network_for_Multi-spectral_Image_Fusion_CVPR_2025_paper.html)

  [Junming Hou](https://openaccess.thecvf.com/CVPR2025#), [Xiaoyu Chen](https://openaccess.thecvf.com/CVPR2025#), [Ran Ran](https://openaccess.thecvf.com/CVPR2025#), [Xiaofeng Cong](https://openaccess.thecvf.com/CVPR2025#), [Xinyang Liu](https://openaccess.thecvf.com/CVPR2025#), [Jian Wei You](https://openaccess.thecvf.com/CVPR2025#), [Liang-Jian Deng](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Hou_Binarized_Neural_Network_for_Multi-spectral_Image_Fusion_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Hou_Binarized_Neural_Network_CVPR_2025_supplemental.pdf)]  

-  [SAIST: Segment Any Infrared Small Target Model Guided by Contrastive Language-Image Pretraining](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SAIST_Segment_Any_Infrared_Small_Target_Model_Guided_by_Contrastive_CVPR_2025_paper.html)

  [Mingjin Zhang](https://openaccess.thecvf.com/CVPR2025#), [Xiaolong Li](https://openaccess.thecvf.com/CVPR2025#), [Fei Gao](https://openaccess.thecvf.com/CVPR2025#), [Jie Guo](https://openaccess.thecvf.com/CVPR2025#), [Xinbo Gao](https://openaccess.thecvf.com/CVPR2025#), [Jing Zhang](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_SAIST_Segment_Any_Infrared_Small_Target_Model_Guided_by_Contrastive_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Zhang_SAIST_Segment_Any_CVPR_2025_supplemental.pdf)]  

-  [V2X-R: Cooperative LiDAR-4D Radar Fusion with Denoising Diffusion for 3D Object Detection](https://openaccess.thecvf.com/content/CVPR2025/html/Huang_V2X-R_Cooperative_LiDAR-4D_Radar_Fusion_with_Denoising_Diffusion_for_3D_CVPR_2025_paper.html)

  [Xun Huang](https://openaccess.thecvf.com/CVPR2025#), [Jinlong Wang](https://openaccess.thecvf.com/CVPR2025#), [Qiming Xia](https://openaccess.thecvf.com/CVPR2025#), [Siheng Chen](https://openaccess.thecvf.com/CVPR2025#), [Bisheng Yang](https://openaccess.thecvf.com/CVPR2025#), [Xin Li](https://openaccess.thecvf.com/CVPR2025#), [Cheng Wang](https://openaccess.thecvf.com/CVPR2025#), [Chenglu Wen](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_V2X-R_Cooperative_LiDAR-4D_Radar_Fusion_with_Denoising_Diffusion_for_3D_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Huang_V2X-R_Cooperative_LiDAR-4D_CVPR_2025_supplemental.pdf)]  

-  [SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion](https://openaccess.thecvf.com/content/CVPR2025/html/Guo_SGFormer_Satellite-Ground_Fusion_for_3D_Semantic_Scene_Completion_CVPR_2025_paper.html)

  [Xiyue Guo](https://openaccess.thecvf.com/CVPR2025#), [Jiarui Hu](https://openaccess.thecvf.com/CVPR2025#), [Junjie Hu](https://openaccess.thecvf.com/CVPR2025#), [Hujun Bao](https://openaccess.thecvf.com/CVPR2025#), [Guofeng Zhang](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_SGFormer_Satellite-Ground_Fusion_for_3D_Semantic_Scene_Completion_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Guo_SGFormer_Satellite-Ground_Fusion_CVPR_2025_supplemental.zip)] [[arXiv](http://arxiv.org/abs/2503.16825)]  

-  [DCEvo: Discriminative Cross-Dimensional Evolutionary Learning for Infrared and Visible Image Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DCEvo_Discriminative_Cross-Dimensional_Evolutionary_Learning_for_Infrared_and_Visible_Image_CVPR_2025_paper.html)

  [Jinyuan Liu](https://openaccess.thecvf.com/CVPR2025#), [Bowei Zhang](https://openaccess.thecvf.com/CVPR2025#), [Qingyun Mei](https://openaccess.thecvf.com/CVPR2025#), [Xingyuan Li](https://openaccess.thecvf.com/CVPR2025#), [Yang Zou](https://openaccess.thecvf.com/CVPR2025#), [Zhiying Jiang](https://openaccess.thecvf.com/CVPR2025#), [Long Ma](https://openaccess.thecvf.com/CVPR2025#), [Risheng Liu](https://openaccess.thecvf.com/CVPR2025#), [Xin Fan](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_DCEvo_Discriminative_Cross-Dimensional_Evolutionary_Learning_for_Infrared_and_Visible_Image_CVPR_2025_paper.pdf)] [[arXiv](http://arxiv.org/abs/2503.17673)]  

-  [Robust-MVTON: Learning Cross-Pose Feature Alignment and Fusion for Robust Multi-View Virtual Try-On](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Robust-MVTON_Learning_Cross-Pose_Feature_Alignment_and_Fusion_for_Robust_Multi-View_CVPR_2025_paper.html)

  [Nannan Zhang](https://openaccess.thecvf.com/CVPR2025#), [Yijiang Li](https://openaccess.thecvf.com/CVPR2025#), [Dong Du](https://openaccess.thecvf.com/CVPR2025#), [Zheng Chong](https://openaccess.thecvf.com/CVPR2025#), [Zhengwentai Sun](https://openaccess.thecvf.com/CVPR2025#), [Jianhao Zeng](https://openaccess.thecvf.com/CVPR2025#), [Yusheng Dai](https://openaccess.thecvf.com/CVPR2025#), [Zhengyu Xie](https://openaccess.thecvf.com/CVPR2025#), [Hairui Zhu](https://openaccess.thecvf.com/CVPR2025#), [Xiaoguang Han](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Robust-MVTON_Learning_Cross-Pose_Feature_Alignment_and_Fusion_for_Robust_Multi-View_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Zhang_Robust-MVTON_Learning_Cross-Pose_CVPR_2025_supplemental.pdf)]  

-  [Bridge Frame and Event: Common Spatiotemporal Fusion for High-Dynamic Scene Optical Flow](https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Bridge_Frame_and_Event_Common_Spatiotemporal_Fusion_for_High-Dynamic_Scene_CVPR_2025_paper.html)

  [Hanyu Zhou](https://openaccess.thecvf.com/CVPR2025#), [Haonan Wang](https://openaccess.thecvf.com/CVPR2025#), [Haoyue Liu](https://openaccess.thecvf.com/CVPR2025#), [Yuxing Duan](https://openaccess.thecvf.com/CVPR2025#), [Yi Chang](https://openaccess.thecvf.com/CVPR2025#), [Luxin Yan](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhou_Bridge_Frame_and_Event_Common_Spatiotemporal_Fusion_for_High-Dynamic_Scene_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Zhou_Bridge_Frame_and_CVPR_2025_supplemental.zip)] [[arXiv](http://arxiv.org/abs/2503.06992)]  

-  [Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos](https://openaccess.thecvf.com/content/CVPR2025/html/Tschernezki_Layered_Motion_Fusion_Lifting_Motion_Segmentation_to_3D_in_Egocentric_CVPR_2025_paper.html)

  [Vadim Tschernezki](https://openaccess.thecvf.com/CVPR2025#), [Diane Larlus](https://openaccess.thecvf.com/CVPR2025#), [Iro Laina](https://openaccess.thecvf.com/CVPR2025#), [Andrea Vedaldi](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Tschernezki_Layered_Motion_Fusion_Lifting_Motion_Segmentation_to_3D_in_Egocentric_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Tschernezki_Layered_Motion_Fusion_CVPR_2025_supplemental.pdf)]  

-  [Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model](https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Self-Learning_Hyperspectral_and_Multispectral_Image_Fusion_via_Adaptive_Residual_Guided_CVPR_2025_paper.html)

  [Jian Zhu](https://openaccess.thecvf.com/CVPR2025#), [He Wang](https://openaccess.thecvf.com/CVPR2025#), [Yang Xu](https://openaccess.thecvf.com/CVPR2025#), [Zebin Wu](https://openaccess.thecvf.com/CVPR2025#), [Zhihui Wei](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Self-Learning_Hyperspectral_and_Multispectral_Image_Fusion_via_Adaptive_Residual_Guided_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Zhu_Self-Learning_Hyperspectral_and_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2505.11800)]  

-  [Resilient Sensor Fusion Under Adverse Sensor Failures via Multi-Modal Expert Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Park_Resilient_Sensor_Fusion_Under_Adverse_Sensor_Failures_via_Multi-Modal_Expert_CVPR_2025_paper.html)

  [Konyul Park](https://openaccess.thecvf.com/CVPR2025#), [Yecheol Kim](https://openaccess.thecvf.com/CVPR2025#), [Daehun Kim](https://openaccess.thecvf.com/CVPR2025#), [Jun Won Choi](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Park_Resilient_Sensor_Fusion_Under_Adverse_Sensor_Failures_via_Multi-Modal_Expert_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Park_Resilient_Sensor_Fusion_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2503.19776)]  

-  [Task-driven Image Fusion with Learnable Fusion Loss](https://openaccess.thecvf.com/content/CVPR2025/html/Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss_CVPR_2025_paper.html)

  [Haowen Bai](https://openaccess.thecvf.com/CVPR2025#), [Jiangshe Zhang](https://openaccess.thecvf.com/CVPR2025#), [Zixiang Zhao](https://openaccess.thecvf.com/CVPR2025#), [Yichen Wu](https://openaccess.thecvf.com/CVPR2025#), [Lilun Deng](https://openaccess.thecvf.com/CVPR2025#), [Yukun Cui](https://openaccess.thecvf.com/CVPR2025#), [Tao Feng](https://openaccess.thecvf.com/CVPR2025#), [Shuang Xu](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Bai_Task-driven_Image_Fusion_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2412.03240)]  

-  [Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty Fusion in Autonomous Vehicles Perception](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Hyperdimensional_Uncertainty_Quantification_for_Multimodal_Uncertainty_Fusion_in_Autonomous_Vehicles_CVPR_2025_paper.html)

  [Luke Chen](https://openaccess.thecvf.com/CVPR2025#), [Junyao Wang](https://openaccess.thecvf.com/CVPR2025#), [Trier Mortlock](https://openaccess.thecvf.com/CVPR2025#), [Pramod Khargonekar](https://openaccess.thecvf.com/CVPR2025#), [Mohammad Abdullah Al Faruque](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Hyperdimensional_Uncertainty_Quantification_for_Multimodal_Uncertainty_Fusion_in_Autonomous_Vehicles_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Chen_Hyperdimensional_Uncertainty_Quantification_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2503.20011)]  

-  [Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D Semantic Occupancy Prediction](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Rethinking_Temporal_Fusion_with_a_Unified_Gradient_Descent_View_for_CVPR_2025_paper.html)

  [Dubing Chen](https://openaccess.thecvf.com/CVPR2025#), [Huan Zheng](https://openaccess.thecvf.com/CVPR2025#), [Jin Fang](https://openaccess.thecvf.com/CVPR2025#), [Xingping Dong](https://openaccess.thecvf.com/CVPR2025#), [Xianfei Li](https://openaccess.thecvf.com/CVPR2025#), [Wenlong Liao](https://openaccess.thecvf.com/CVPR2025#), [Tao He](https://openaccess.thecvf.com/CVPR2025#), [Pai Peng](https://openaccess.thecvf.com/CVPR2025#), [Jianbing Shen](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Rethinking_Temporal_Fusion_with_a_Unified_Gradient_Descent_View_for_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Chen_Rethinking_Temporal_Fusion_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2504.12959)]  

-  [Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Tokenize_Image_Patches_Global_Context_Fusion_for_Effective_Haze_Removal_CVPR_2025_paper.html)

  [Jiuchen Chen](https://openaccess.thecvf.com/CVPR2025#), [Xinyu Yan](https://openaccess.thecvf.com/CVPR2025#), [Qizhi Xu](https://openaccess.thecvf.com/CVPR2025#), [Kaiqi Li](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Tokenize_Image_Patches_Global_Context_Fusion_for_Effective_Haze_Removal_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Chen_Tokenize_Image_Patches_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2504.09621)]  

-  [SVDC: Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SVDC_Consistent_Direct_Time-of-Flight_Video_Depth_Completion_with_Frequency_Selective_CVPR_2025_paper.html)

  [Xuan Zhu](https://openaccess.thecvf.com/CVPR2025#), [Jijun Xiang](https://openaccess.thecvf.com/CVPR2025#), [Xianqi Wang](https://openaccess.thecvf.com/CVPR2025#), [Longliang Liu](https://openaccess.thecvf.com/CVPR2025#), [Yu Wang](https://openaccess.thecvf.com/CVPR2025#), [Hong Zhang](https://openaccess.thecvf.com/CVPR2025#), [Fei Guo](https://openaccess.thecvf.com/CVPR2025#), [Xin Yang](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_SVDC_Consistent_Direct_Time-of-Flight_Video_Depth_Completion_with_Frequency_Selective_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Zhu_SVDC_Consistent_Direct_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2503.01257)]  

-  [Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis](https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Exploring_the_Deep_Fusion_of_Large_Language_Models_and_Diffusion_CVPR_2025_paper.html)

  [Bingda Tang](https://openaccess.thecvf.com/CVPR2025#), [Boyang Zheng](https://openaccess.thecvf.com/CVPR2025#), [Sayak Paul](https://openaccess.thecvf.com/CVPR2025#), [Saining Xie](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Tang_Exploring_the_Deep_Fusion_of_Large_Language_Models_and_Diffusion_CVPR_2025_paper.pdf)] [[arXiv](http://arxiv.org/abs/2505.10046)]  

-  [Pseudo Visible Feature Fine-Grained Fusion for Thermal Object Detection](https://openaccess.thecvf.com/content/CVPR2025/html/Li_Pseudo_Visible_Feature_Fine-Grained_Fusion_for_Thermal_Object_Detection_CVPR_2025_paper.html)

  [Ting Li](https://openaccess.thecvf.com/CVPR2025#), [Mao Ye](https://openaccess.thecvf.com/CVPR2025#), [Tianwen Wu](https://openaccess.thecvf.com/CVPR2025#), [Nianxin Li](https://openaccess.thecvf.com/CVPR2025#), [Shuaifeng Li](https://openaccess.thecvf.com/CVPR2025#), [Song Tang](https://openaccess.thecvf.com/CVPR2025#), [Luping Ji](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Pseudo_Visible_Feature_Fine-Grained_Fusion_for_Thermal_Object_Detection_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Li_Pseudo_Visible_Feature_CVPR_2025_supplemental.pdf)]  

-  [SeqMvRL: A Sequential Fusion Framework for Multi-view Representation Learning](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SeqMvRL_A_Sequential_Fusion_Framework_for_Multi-view_Representation_Learning_CVPR_2025_paper.html)

  [Ren Wang](https://openaccess.thecvf.com/CVPR2025#), [Haoliang Sun](https://openaccess.thecvf.com/CVPR2025#), [Yuxiu Lin](https://openaccess.thecvf.com/CVPR2025#), [Chuanhui Zuo](https://openaccess.thecvf.com/CVPR2025#), [Yongshun Gong](https://openaccess.thecvf.com/CVPR2025#), [Yilong Yin](https://openaccess.thecvf.com/CVPR2025#), [Wenjia Meng](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_SeqMvRL_A_Sequential_Fusion_Framework_for_Multi-view_Representation_Learning_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_SeqMvRL_A_Sequential_CVPR_2025_supplemental.pdf)]  

-  [Enhanced then Progressive Fusion with View Graph for Multi-View Clustering](https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Enhanced_then_Progressive_Fusion_with_View_Graph_for_Multi-View_Clustering_CVPR_2025_paper.html)

  [Zhibin Dong](https://openaccess.thecvf.com/CVPR2025#), [Meng Liu](https://openaccess.thecvf.com/CVPR2025#), [Siwei Wang](https://openaccess.thecvf.com/CVPR2025#), [Ke Liang](https://openaccess.thecvf.com/CVPR2025#), [Yi Zhang](https://openaccess.thecvf.com/CVPR2025#), [Suyuan Liu](https://openaccess.thecvf.com/CVPR2025#), [Jiaqi Jin](https://openaccess.thecvf.com/CVPR2025#), [Xinwang Liu](https://openaccess.thecvf.com/CVPR2025#), [En Zhu](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Dong_Enhanced_then_Progressive_Fusion_with_View_Graph_for_Multi-View_Clustering_CVPR_2025_paper.pdf)]  

-  [SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SocialMOIF_Multi-Order_Intention_Fusion_for_Pedestrian_Trajectory_Prediction_CVPR_2025_paper.html)

  [Kai Chen](https://openaccess.thecvf.com/CVPR2025#), [Xiaodong Zhao](https://openaccess.thecvf.com/CVPR2025#), [Yujie Huang](https://openaccess.thecvf.com/CVPR2025#), [Guoyu Fang](https://openaccess.thecvf.com/CVPR2025#), [Xiao Song](https://openaccess.thecvf.com/CVPR2025#), [Ruiping Wang](https://openaccess.thecvf.com/CVPR2025#), [Ziyuan Wang](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_SocialMOIF_Multi-Order_Intention_Fusion_for_Pedestrian_Trajectory_Prediction_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Chen_SocialMOIF_Multi-Order_Intention_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2504.15616)]  

-  [Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2025/html/Qorbani_Semantic_Library_Adaptation_LoRA_Retrieval_and_Fusion_for_Open-Vocabulary_Semantic_CVPR_2025_paper.html)

  [Reza Qorbani](https://openaccess.thecvf.com/CVPR2025#), [Gianluca Villani](https://openaccess.thecvf.com/CVPR2025#), [Theodoros Panagiotakopoulos](https://openaccess.thecvf.com/CVPR2025#), [Marc Botet Colomer](https://openaccess.thecvf.com/CVPR2025#), [Linus Härenstam-Nielsen](https://openaccess.thecvf.com/CVPR2025#), [Mattia Segu](https://openaccess.thecvf.com/CVPR2025#), [Pier Luigi Dovesi](https://openaccess.thecvf.com/CVPR2025#), [Jussi Karlgren](https://openaccess.thecvf.com/CVPR2025#), [Daniel Cremers](https://openaccess.thecvf.com/CVPR2025#), [Federico Tombari](https://openaccess.thecvf.com/CVPR2025#), [Matteo Poggi](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Qorbani_Semantic_Library_Adaptation_LoRA_Retrieval_and_Fusion_for_Open-Vocabulary_Semantic_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Qorbani_Semantic_Library_Adaptation_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2503.21780)]  

-  [MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds](https://openaccess.thecvf.com/content/CVPR2025/html/Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion_CVPR_2025_paper.html)

  [Jiahui Lei](https://openaccess.thecvf.com/CVPR2025#), [Yijia Weng](https://openaccess.thecvf.com/CVPR2025#), [Adam W. Harley](https://openaccess.thecvf.com/CVPR2025#), [Leonidas Guibas](https://openaccess.thecvf.com/CVPR2025#), [Kostas Daniilidis](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Lei_MoSca_Dynamic_Gaussian_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2405.17421)]  

-  [Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAV Target Detection](https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target_CVPR_2025_paper.html)

  [Houzhang Fang](https://openaccess.thecvf.com/CVPR2025#), [Xiaolin Wang](https://openaccess.thecvf.com/CVPR2025#), [Zengyang Li](https://openaccess.thecvf.com/CVPR2025#), [Lu Wang](https://openaccess.thecvf.com/CVPR2025#), [Qingshan Li](https://openaccess.thecvf.com/CVPR2025#), [Yi Chang](https://openaccess.thecvf.com/CVPR2025#), [Luxin Yan](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Fang_Detection-Friendly_Nonuniformity_Correction_CVPR_2025_supplemental.pdf)]  

-  [EditSplat: Multi-View Fusion and Attention-Guided Optimization for View-Consistent 3D Scene Editing with 3D Gaussian Splatting](https://openaccess.thecvf.com/content/CVPR2025/html/Lee_EditSplat_Multi-View_Fusion_and_Attention-Guided_Optimization_for_View-Consistent_3D_Scene_CVPR_2025_paper.html)

  [Dong In Lee](https://openaccess.thecvf.com/CVPR2025#), [Hyeongcheol Park](https://openaccess.thecvf.com/CVPR2025#), [Jiyoung Seo](https://openaccess.thecvf.com/CVPR2025#), [Eunbyung Park](https://openaccess.thecvf.com/CVPR2025#), [Hyunje Park](https://openaccess.thecvf.com/CVPR2025#), [Ha Dam Baek](https://openaccess.thecvf.com/CVPR2025#), [Sangheon Shin](https://openaccess.thecvf.com/CVPR2025#), [Sangmin Kim](https://openaccess.thecvf.com/CVPR2025#), [Sangpil Kim](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Lee_EditSplat_Multi-View_Fusion_and_Attention-Guided_Optimization_for_View-Consistent_3D_Scene_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Lee_EditSplat_Multi-View_Fusion_CVPR_2025_supplemental.pdf)] [[arXiv](http://arxiv.org/abs/2412.11520)]  

-  [UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion_CVPR_2025_paper.html)

  [Zixuan Chen](https://openaccess.thecvf.com/CVPR2025#), [Yujin Wang](https://openaccess.thecvf.com/CVPR2025#), [Xin Cai](https://openaccess.thecvf.com/CVPR2025#), [Zhiyuan You](https://openaccess.thecvf.com/CVPR2025#), [Zheming Lu](https://openaccess.thecvf.com/CVPR2025#), [Fan Zhang](https://openaccess.thecvf.com/CVPR2025#), [Shi Guo](https://openaccess.thecvf.com/CVPR2025#), [Tianfan Xue](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion_CVPR_2025_paper.pdf)] [[supp](https://openaccess.thecvf.com/content/CVPR2025/supplemental/Chen_UltraFusion_Ultra_High_CVPR_2025_supplemental.zip)] [[arXiv](http://arxiv.org/abs/2501.11515)]  

-  [DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared Imaging](https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DEAL_Data-Efficient_Adversarial_Learning_for_High-Quality_Infrared_Imaging_CVPR_2025_paper.html)

  [Zhu Liu](https://openaccess.thecvf.com/CVPR2025#), [Zijun Wang](https://openaccess.thecvf.com/CVPR2025#), [Jinyuan Liu](https://openaccess.thecvf.com/CVPR2025#), [Fanqi Meng](https://openaccess.thecvf.com/CVPR2025#), [Long Ma](https://openaccess.thecvf.com/CVPR2025#), [Risheng Liu](https://openaccess.thecvf.com/CVPR2025#)

  [[pdf](https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_DEAL_Data-Efficient_Adversarial_Learning_for_High-Quality_Infrared_Imaging_CVPR_2025_paper.pdf)] [[arXiv](http://arxiv.org/abs/2503.00905)]  

​    

## 综述(Survey)

| 标题                                                         | 论文                                                         | 代码                                           | 发表期刊或会议 | 年份 |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------- | -------------- | ---- |
| A review of  remote sensing image fusion methods             | [Paper](https://www.sciencedirect.com/science/article/pii/S1566253516300173?casa_token=1JHuWMUO20QAAAAA:gzRSAmDvI5iciNDoihkI8raYANyPb6PzXE-f9U5iE9LUh9FIsfOiMBLK4VFyA6l84eDIV9HOIKs6) |                                                | InFus          | 2016 |
| Pixel-level  image fusion: A survey of the state of the art  | [Paper](https://www.sciencedirect.com/science/article/pii/S1566253516300458?casa_token=8TemDHLkC_oAAAAA:ucXl6DUaMy9PpEnRueneNDpDBMeDy8P5M6JAaLkhg1Y6dvA9fdFHnuq9aXMapfMgfC18Ffa5eT07) |                                                | InFus          | 2017 |
| Deep learning for pixel-level image fusion: Recent advances and future prospects | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S1566253517305936?casa_token=PPYorDBH2wAAAAAA:6FssewzwcKsdfVDQGoQH9Uu154HtjOTw05vXK2S7c3g02QzRLroC1gJ7MXwdMCwqsC2Mx9UWgEx5) |                                                | InFus          | 2018 |
| Infrared and visible image fusion methods and applications: A survey | [Paper](https://www.sciencedirect.com/science/article/pii/S1566253517307972?casa_token=HpeaaU2oKNIAAAAA:MgWpCued5JhnK3KDBLQ9dbpzSORbThpSfTPXFGJDqYEMCl6C1nmTI63MG_FzaDBnGPmJKUxW-KAt) |                                                | InFus          | 2019 |
| Multi-focu image fusion: A Survey of the state of the art    | [Paper](https://www.sciencedirect.com/science/article/pii/S1566253520303109?casa_token=yugI5Tt6HAMAAAAA:jT4uV1xK2HjtfZWPuh2FkcqtHC1CN5TAYGR64ZWika5B0xAro9r-S5qOk30UDrNziKDuWPLQIsWw) |                                                | InFus          | 2020 |
| Image fusion meets deep learning: A survey and perspective   | [Paper](https://www.sciencedirect.com/science/article/pii/S1566253521001342?casa_token=4BDX2mCr4VwAAAAA:LzXkYoP1QwTW5lXEpMCdVEZ2W27ZXf8VbJIVOX10dl-SpYuuYkNyaTk7uiD5JCjSbn6grf8DXy33) |                                                | InFus          | 2021 |
| Deep Learning-based Multi-focus Image Fusion: A Survey and A Comparative  Study | [Paper](https://ieeexplore.ieee.org/document/9428544/)       | [Code](https://github.com/xingchenzhang/MFIFB) | TPAMI          | 2021 |
| Benchmarking and comparing multi-exposure image fusion algorithms | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S1566253521000233?casa_token=3fZWWc9-l_kAAAAA:en9qngjXa_neaqkNdp73a25flyglW2IzNkjuYSqUYh4ZxvZy63n48-pzPboX1p2I95ofJxiz2ptp) | [Code](https://github.com/xingchenzhang/MEFB)  | InFus          | 2021 |
| Current advances and future perspectives of image fusion: A comprehensive review | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S1566253522001518) | [Code]()                                       | InFus          | 2023 |





### before 2018

- Multispectral Pedestrian Detection Benchmark Dataset and Baseline, 2015, Soonmin Hwang et al.
  [[PDF](https://soonminhwang.github.io/rgbt-ped-detection/misc/CVPR15_Pedestrian_Benchmark.pdf)]
  [[Code](https://github.com/SoonminHwang/rgbt-ped-detection)]

- Multispectral Pedestrian Detection using Deep Fusion Convolutional Neural Networks, 2016, Jörg Wagner et al.
  [[PDF](https://www.researchgate.net/publication/302514661_Multispectral_Pedestrian_Detection_using_Deep_Fusion_Convolutional_Neural_Networks)]

- Multispectral Deep Neural Networks for Pedestrian Detection, 2016, Jingjing Liu et al.
  [[PDF](https://arxiv.org/abs/1611.02644)]
  [[Code](https://github.com/denny1108/multispectral-pedestrian-py-faster-rcnn)]

- Multi-spectral Pedestrian Detection Based on Accumulated Object Proposal with Fully Convolutional Networks, 2016, Hangil Choi et al.
  [[PDF](https://ieeexplore.ieee.org/document/7899703)]

- Fully Convolutional Region Proposal Networks for Multispectral Person Detection, 2017, Daniel König et al.
  [[PDF](https://ieeexplore.ieee.org/abstract/document/8014770)]

- Unified Multi-spectral Pedestrian Detection Based on Probabilistic Fusion Networks, 2017, Kihong Park et al.
  [[PDF](https://www.sciencedirect.com/science/article/abs/pii/S0031320318300906)]

### 2018

- Fusion of Multispectral Data Through Illumination-aware Deep Neural Networks for Pedestrian Detection, 2018, Dayan Guan et al.
  [[PDF](https://arxiv.org/abs/1802.09972)]
  [[Code](https://github.com/dayanguan/illumination-aware_multispectral_pedestrian_detection/)]

- Illumination-aware Faster R-CNN for Robust Multispectral Pedestrian Detection, BMVC 2018, Chengyang Li et al.
  [[PDF](https://arxiv.org/pdf/1802.09972.pdf)]
  [[Code](https://github.com/Li-Chengyang/IAF-RCNN)]

- Pedestrian detection at night by using Faster R-CNN infrared images, 2018, Michelle Galarza Bravo et al.
  [[PDF](https://ingenius.ups.edu.ec/index.php/ingenius/article/download/20.2018.05/2767)]

- Real-Time Multispectral Pedestrian Detection with a Single-Pass Deep Neural Network, 2018, Maarten Vandersteegen et al.
  [[PDF](https://link.springer.com/chapter/10.1007/978-3-319-93000-8_47)]

- Multispectral Pedestrian Detection via Simultaneous Detection and Segmentation, BMVC 2018, Chengyang Li et al.
  [[PDF](https://arxiv.org/abs/1808.04818)]
  [[Code](https://github.com/Li-Chengyang/MSDS-RCNN)]
  [[Project Link](https://li-chengyang.github.io/home/MSDS-RCNN/)]

### 2019

- Box-level Segmentation Supervised Deep Neural Networks for Accurate and Real-time Multispectral Pesdestrian Detecion, 2019, Yanpeng Cao et al.
  [[PDF](https://arxiv.org/abs/1902.05291)]
  [[Code](https://github.com/dayanguan/realtime_multispectral_pedestrian_detection)]

 - Weakly Aligned Cross-Modal Learning for Multispectral Pedestrian Detection, ICCV 2019, Lu Zhang et al.
   [[PDF](https://arxiv.org/abs/1901.02645)]
   [[Code](https://github.com/luzhang16/AR-CNN)]

- The Cross-Modality Disparity Problem in Multispectral Pedestrian Detection, 2019, Lu Zhang et al.
  [[PDF](https://arxiv.org/abs/1901.02645v1)]

- Cross-modality interactive attention network for multispectral pedestrian, 2019, Lu Zhang et al.
  [[PDF](https://www.sciencedirect.com/science/article/abs/pii/S1566253518304111)]
  [[Code](https://github.com/luzhang16/CIAN)]

- GFD-SSD  Gated Fusion Double SSD for Multispectral Pedestrian Detection, 2019, Yang Zheng et al.
  [[PDF](https://arxiv.org/abs/1903.06999)]

- Unsupervised Domain Adaptation for Multispectral Pedestrian Detection, 2019, Dayan Guan et al.
  [[PDF](https://arxiv.org/abs/1904.03692)]
  [[Code](https://github.com/dayanguan/unsupervised_multispectral_pedestrian_detectio)]

- Generalization ability of region proposal networks for multispectral person detection, 2019, Kevin Fritz et al.[[PDF](https://arxiv.org/abs/1905.02758)]

- Borrow from Anywhere: Pseudo Multi-modal Object Detection in Thermal Imagery, 2019, Chaitanya Devaguptapu et al. [[PDF](https://arxiv.org/abs/1905.08789)]

### 2020

- Multispectral Fusion for Object Detection with Cyclic Fuse-and-Refine Blocks, ICIP 2020, Heng Zhang et al. [[PDF](https://hal.archives-ouvertes.fr/hal-02872132/file/icip2020.pdf)]

- Improving Multispectral Pedestrian Detection by Addressing Modality Imbalance Problems, ECCV 2020, Kailai Zhou et al. [[PDF](https://arxiv.org/pdf/2008.03043.pdf)][[Code](https://github.com/CalayZhou/MBNet)]

- Task-conditioned Domain Adaptation for Pedestrian Detection in Thermal Imagery, ECCV 2020, My Kieu et al. [[PDF](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670545.pdf)]

- Anchor-free Small-scale Multispectral Pedestrian Detection, BMVC 2020, Alexander Wolpert et al. [[PDF](https://arxiv.org/abs/2008.08418)][[Code](https://github.com/HensoldtOptronicsCV/MultispectralPedestrianDetection)]

- Robust pedestrian detection in thermal imagery using synthesized images, ICPR 2020, My Kieu et al.[[PDF](https://arxiv.org/abs/2102.02005)]

### 2021

- Pixel Invisibility: Detecting Objects Invisible in Color Image, 2021, Yongxin Wang et al.[[PDF](https://arxiv.org/pdf/2006.08383.pdf)]

- Guided Attentive Feature Fusion for Multispectral Pedestrian Detection, WACV 2021, Heng Zhang et al. [[PDF](https://openaccess.thecvf.com/content/WACV2021/papers/Zhang_Guided_Attentive_Feature_Fusion_for_Multispectral_Pedestrian_Detection_WACV_2021_paper.pdf)]

- Deep Active Learning from Multispectral Data Through Cross-Modality Prediction Inconsistency, ICIP2021, Heng Zhang et al. [[PDF](https://hal.archives-ouvertes.fr/hal-03236409/document)]

- Spatio-Contextual Deep Network Based Multimodal Pedestrian Detection For Autonomous Driving, Kinjal Dasgupta et al. [[PDF](https://arxiv.org/abs/2105.12713)]

- Uncertainty-Guided Cross-Modal Learning for Robust Multispectral Pedestrian Detection, IEEE Transactions on Circuits and Systems for Video Technology 2021, Jung Uk Kim et al. [[PDF](https://ieeexplore.ieee.org/document/9419080)]

- Cross-Modality Fusion Transformer for Multispectral Object Detection, 2021, Qingyun Fang et al. [[PDF](https://arxiv.org/pdf/2111.00273v2.pdf)]

- Weakly Aligned Feature Fusion for Multimodal Object Detection, 2021, Lu Zhang et al.  [[PDF](https://ieeexplore.ieee.org/abstract/document/9523596)]

- Attention Fusion for One-Stage Multispectral Pedestrian Detection, 2021, Zhiwei Cao et al. [[PDF](https://www.mdpi.com/1424-8220/21/12/4184)]

- Multi-Modal Pedestrian Detection with Large Misalignment Based on Modal-Wise Regression and Multi-Modal IoU, 2021, Napat Wanchaitanawong et al. [[PDF](https://arxiv.org/pdf/2107.11196.pdf)]

- MLPD: Multi-Label Pedestrian Detector in Multispectral Domain, 2021, Jiwon Kim et al. [[PDF](https://ieeexplore.ieee.org/document/9496129)]

- [survey] From handcrafted to deep features for pedestrian detection: a survey, IEEE Transactions on Pattern Analysis and Machine Intelligence 2021, Jiale Cao et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9420291)]

### 2022

- Low-Cost Multispectral Scene Analysis With Modality Distillation, WACV 2022, Heng Zhang et al. [[PDF](https://openaccess.thecvf.com/content/WACV2022/html/Zhang_Low-Cost_Multispectral_Scene_Analysis_With_Modality_Distillation_WACV_2022_paper.html)]

- Confidence-aware Fusion using Dempster-Shafer Theory for Multispectral Pedestrian Detection, IEEE Transactions on Multimedia 2022, Qing Li et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9739079)]

- PIAFusion: A progressive infrared and visible image fusion network based on illumination aware, Information Fusion, Linfeng Tang et al. [[PDF](https://www.sciencedirect.com/science/article/pii/S156625352200032X)]  [[Code](https://github.com/Linfeng-Tang/PIAFusion)]

- Improving RGB-Infrared Object Detection by Reducing Cross-Modality Redundancy, Remote Sensing, Qingwang Wang et al. [[PDF](https://www.mdpi.com/2072-4292/14/9/2020)]

- Spatio-contextual deep network-based multimodal pedestrian detection for autonomous driving, IEEE Transactions on Intelligent Transportation Systems, Kinjal Dasgupta et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9706418)]

- Robust Thermal Infrared Pedestrian Detection By Associating Visible Pedestrian Knowledge, ICASSP 2022, Sungjune Park et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9746886)]

- Drone-based RGB-Infrared Cross-Modality Vehicle Detection via Uncertainty-Aware Learning, IEEE Transactions on Circuits and Systems for Video Technology, Yiming Sun. [[PDF](https://ieeexplore.ieee.org/abstract/document/9759286)] [[Code](https://github.com/VisDrone/DroneVehicle)]

- Towards Versatile Pedestrian Detector with Multisensory-Matching and Multispectral Recalling Memory, AAAI2022, Jung Uk Kim et al. [[PDF](https://www.aaai.org/AAAI22Papers/AAAI-8768.KimJ.pdf)]

- Bispectral Pedestrian Detection Augmented with Saliency Maps using Transformer, VISIGRAPP2022, Mohamed Amine Marnissi et al. [[PDF](https://pdfs.semanticscholar.org/bd9f/468d9f8c6b724ebb369eaf69a8c979f15209.pdf)]

- Attention-Guided Multi-modal and Multi-scale Fusion for Multispectral Pedestrian Detection, PRCV2022, Wei Bao et al. [[PDF](https://link.springer.com/chapter/10.1007/978-3-031-18907-4_30)]

- Improving Rgb-Infrared Pedestrian Detection by Reducing Cross-Modality Redundancy, ICIP2022, Qingwang Wang et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9897682)]

- Attention-Based Cross-Modality Feature Complementation for Multispectral Pedestrian Detection, IEEE Access, Qunyan Jiang et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9775119)]

- DMFFNet: Dual-Mode Multi-Scale Feature Fusion-Based Pedestrian Detection Method,  IEEE Access, Ruizhe Hu et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9805743)]

- LGADet: Light-weight Anchor-free Multispectral Pedestrian Detection with Mixed Local and Global Attention, Neural Processing Letters, Xin Zuo et al. [[PDF](https://link.springer.com/article/10.1007/s11063-022-10991-7)]

- Locality guided cross-modal feature aggregation and pixel-level fusion for multispectral pedestrian detection, Information Fusion, Yanpeng Cao et al. [[PDF](https://www.sciencedirect.com/science/article/pii/S1566253522000549)]

- BAANet: Learning Bi-directional Adaptive Attention Gates for Multispectral Pedestrian Detection, ICRA2022, Xiaoxiao Yang et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9811999)]

- RGB-Thermal based Pedestrian Detection with Single-Modal Augmentation and ROI Pooling Multiscale Fusion, IGARSS2022, Jiajun Xiang et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9883131)]

- MPDFF: Multi-source Pedestrian detection based on Feature Fusion, IGARSS2022, Lingxuan Meng et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9884864)]

- Modality-Independent Regression and Training for Improving Multispectral Pedestrian Detection, ICIVC2022, Han Ni et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/9887331)]

- Learning a Dynamic Cross-Modal Network for Multispectral Pedestrian Detection, ACM MM2022, Jin Xie et al. [[PDF](https://dl.acm.org/doi/abs/10.1145/3503161.3547895)]

- Multimodal Object Detection via Probabilistic Ensembling, ECCV2022(oral), Yi-Ting Chen et al. [[PDF](https://arxiv.org/abs/2104.02904.pdf)] [[Code](https://github.com/Jamie725/Multimodal-Object-Detection-via-Probabilistic-Ensembling)]

- Translation, Scale and Rotation Cross-Modal Alignment Meets RGB-Infrared Vehicle Detection, ECCV2022, Yuan Maoxun et al. [[PDF](https://arxiv.org/abs/2209.13801)] 

### 2023

- [survey] RGB-T image analysis technology and application: A survey, Engineering Applications of Artificial Intelligence, Kechen Song et al. [[PDF](https://www.sciencedirect.com/science/article/pii/S0952197623001033)] 
- [survey] Visible-infrared cross-modal pedestrian detection: a summary, Qian Bie et al. [[PDF](http://www.cjig.cn/jig/ch/reader/view_abstract.aspx?file_no=220670&flag=1)] 
- HAFNet: Hierarchical Attentive Fusion Network for Multispectral Pedestrian Detection,  Remote Sensing, Peiran Peng et al. [[PDF](https://www.mdpi.com/2072-4292/15/8/2041)] 
- Local Adaptive Illumination-Driven Input-Level Fusion for Infrared and Visible Object Detection,  Remote Sensing, Jiawen Wu et al. [[PDF](https://www.mdpi.com/2072-4292/15/3/660)] 
- Multiscale Cross-modal Homogeneity Enhancement and Confidence-aware Fusion for Multispectral Pedestrian Detection, IEEE Transactions on Multimedia, Ruimin Li et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/10114594)] 
- Transformer fusion and histogram layer multispectral pedestrian detection network,  Signal, Image and Video Processing, Ying Zang et al. [[PDF](https://link.springer.com/article/10.1007/s11760-023-02579-y)]  
- DaCFN: divide-and-conquer fusion network for RGB-T object detection, International Journal of Machine Learning and Cybernetics, Bofan Wang et al. [[PDF](https://link.springer.com/article/10.1007/s13042-022-01771-9)]  
- Cross-modality complementary information fusion for multispectral pedestrian detection, Neural Computing and Applications, Chaoqi Yan et al. [[PDF](https://link.springer.com/article/10.1007/s00521-023-08239-z)] 
- IGT: Illumination-guided RGB-T object detection with transformers, Knowledge-Based Systems, Keyu Chen et al. [[PDF](https://www.sciencedirect.com/science/article/pii/S0950705123001739)]  
- Learning to measure infrared properties of street views from visible images, Measurement, Lei Wang et al. [[PDF](https://www.sciencedirect.com/science/article/pii/S0263224122015160)]  
- Multispectral Pedestrian Detection via Reference Box Constrained CrossAttention and Modality Balanced Optimization, Yinghui Xing et al. [[PDF](https://arxiv.org/pdf/2302.00290.pdf)]  
- Cascaded information enhancement andcross-modal attention feature fusion formultispectral pedestrian detection, Yang Yang et al. [[PDF](https://arxiv.org/pdf/2302.08670.pdf)]  
- Cross-Modality Attention and Multimodal Fusion Transformer for Pedestrian Detection, ECCV 2022 Workshops, Wei-Yu Lee et al. [[PDF](https://link.springer.com/chapter/10.1007/978-3-031-25072-9_41)]  
- REVISITING MODALITY IMBALANCE IN MULTIMODAL PEDESTRIAN DETECTION, Arindam Das et al. [[PDF](https://arxiv.org/pdf/2302.12589.pdf)]  
- Illumination-Guided RGBT Object Detection With Inter- and Intra-Modality Fusion, IEEE Transactions on Instrumentation and Measurement, Yan Zhang et al. [[PDF](https://ieeexplore.ieee.org/abstract/document/10057437)]  
- MCANet: Multiscale Cross-Modality Attention Network for Multispectral Pedestrian Detection,  MultiMedia Modeling, Xiaotian Wang et al. [[PDF](https://link.springer.com/chapter/10.1007/978-3-031-27077-2_4)]  
- Multi-modal pedestrian detection with misalignment based on modal-wise regression and multi-modal IoU, Journal of Electronic Imaging, Napat Wanchaitanawong et al. [[PDF](https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-32/issue-1/013025/Multi-modal-pedestrian-detection-with-misalignment-based-on-modal-wise/10.1117/1.JEI.32.1.013025.full?SSO=1)]  


### 2024

- Causal Mode Multiplexer: A Novel Framework for Unbiased Multispectral Pedestrian Detection, CVPR 2024, Taeheon Kim et al.  [[PDF](https://arxiv.org/pdf/2403.01300.pdf))] [[code](https://github.com/ssbin0914/Causal-Mode-Multiplexer)]  

- HalluciDet: Hallucinating RGB Modality for Person Detection Through Privileged Information, WACV 2024, Medeiros, Heitor Rapela, et al. [[PDF](https://openaccess.thecvf.com/content/WACV2024/html/Medeiros_HalluciDet_Hallucinating_RGB_Modality_for_Person_Detection_Through_Privileged_Information_WACV_2024_paper.html)] [[code](https://github.com/heitorrapela/HalluciDet)]

- MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection, ARXIV 2024, Medeiros, Heitor Rapela, et al.  [[PDF](https://arxiv.org/abs/2404.18849)] [[code](https://github.com/heitorrapela/MiPa)]  

- Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge, ECCV 2024, Medeiros, Heitor Rapela, et al.  [[PDF](https://arxiv.org/pdf/2404.01492)] [[code](https://github.com/heitorrapela/ModTr)]

- When Pedestrian Detection Meets Multi-Modal Learning: Generalist Model and Benchmark Dataset, ECCV 2024, Yi Zhang et al.  [[PDF](https://arxiv.org/abs/2407.10125)] [[code](https://github.com/BubblyYi/MMPedestron)]



